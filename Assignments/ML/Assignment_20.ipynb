{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daff0fb8",
   "metadata": {},
   "source": [
    "# Assignment_20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dced5dde",
   "metadata": {},
   "source": [
    "### 1. What is the underlying concept of Support Vector Machines?\n",
    "\n",
    "**Ans.** The fundamental idea behind Support Vector Machines is to fit the widest possible “street” between the classes. In other words, the goal is to have the largest possible margin between the decision boundary that separates the two classes and the training instances. When performing soft margin classification, the SVM searches for a compromise between perfectly separating the two classes and having the widest possible street (i.e., a few instances may end up on the street). Another key idea is to use kernels when training on nonlinear datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db3adf",
   "metadata": {},
   "source": [
    "### 2. What is the concept of a support vector?\n",
    "\n",
    "**Ans.** The fundamental idea behind Support Vector Machines is to fit the widest possible “street” between the classes. In other words, the goal is to have the largest possible margin between the decision boundary that separates the two classes and the training instances. When performing soft margin classification, the SVM searches for a compromise between perfectly separating the two classes and having the widest possible street (i.e., a few instances may end up on the street). Another key idea is to use kernels when training on nonlinear datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2280665f",
   "metadata": {},
   "source": [
    "### 3. When using SVMs, why is it necessary to scale the inputs?\n",
    "\n",
    "**Ans.** SVMs try to fit the largest possible “street” between the classes (see the first answer), so if the training set is not scaled, the SVM will tend to neglect small features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd5bd12",
   "metadata": {},
   "source": [
    "### 4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?\n",
    "\n",
    "**Ans.** An SVM classifier can output the distance between the test instance and the decision boundary, and you can use this as a confidence score. However, this score cannot be directly converted into an estimation of the class probability. If you set probability=True when creating an SVM in Scikit-Learn, then after training it will calibrate the probabilities using Logistic Regression on the SVM’s scores (trained by an additional five-fold cross-validation on the training data). This will add the predict_proba() and predict_log_proba() methods to the SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e974b2",
   "metadata": {},
   "source": [
    "### 5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual form of the SVM problem?\n",
    "\n",
    "**Ans.** This question applies only to linear SVMs since kernelized can only use the dual form. The computational complexity of the primal form of the SVM problem is proportional to the number of training instances m, while the computational complexity of the dual form is proportional to a number between m2 and m3. So if there are millions of instances, you should definitely use the primal form, because the dual form will be much too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d8c8c",
   "metadata": {},
   "source": [
    "### 6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is it better to raise or lower (gamma)? What about the letter C?\n",
    "\n",
    "**Ans.** If an SVM classifier trained with an RBF kernel underfits the training set, there might be too much regularization. To decrease it, you need to increase gamma or C (or both)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5c7e6a",
   "metadata": {},
   "source": [
    "### 7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters (H, f, A, and b) be set?\n",
    "\n",
    "**Ans.** Let’s call the QP parameters for the hard-margin problem H′, f′, A′ and b′. The QP parameters for the soft-margin problem have m additional parameters (np = n + 1 + m) and m additional constraints (nc = 2m). They can be defined like so:\n",
    "\n",
    "H is equal to H′, plus m columns of 0s on the right and m rows of 0s at the bottom\n",
    "f is equal to f′ with m additional elements, all equal to the value of the hyperparameter C.\n",
    "b is equal to b′ with m additional elements, all equal to 0.\n",
    "A is equal to A′, with an extra m × m identity matrix Im appended to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60dab17",
   "metadata": {},
   "source": [
    "### 8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. See if you can get them to make a model that is similar to yours.\n",
    "\n",
    "**Ans.** it depends on the dataset and of course a lot of other factors add weight but today in this small post I’ll demonstrate how to use LinearSVC , SVC classifier and SGD classifier via Python code and also compare results for the same dataset.\n",
    "Since classifiers are very sensitive to outliers we need to scale them but before we need to pick the right dataset, for simpler results I’ll showcase the benchmark dataset for all classifiers\n",
    "Let's use the Iris dataset: the Iris Setosa and Iris Versicolor classes are linearly separable.\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = iris[\"target\"]\n",
    "\n",
    "setosa_or_versicolor = (y == 0) | (y == 1)\n",
    "X = X[setosa_or_versicolor]\n",
    "y = y[setosa_or_versicolor]\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "C = 5\n",
    "alpha = 1 / (C * len(X))\n",
    "\n",
    "lin_clf = LinearSVC(loss=\"hinge\", C=C, random_state=42)\n",
    "svm_clf = SVC(kernel=\"linear\", C=C)\n",
    "sgd_clf = SGDClassifier(loss=\"hinge\", learning_rate=\"constant\", eta0=0.001, alpha=alpha,\n",
    "                        max_iter=1000, tol=1e-3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "lin_clf.fit(X_scaled, y)\n",
    "svm_clf.fit(X_scaled, y)\n",
    "sgd_clf.fit(X_scaled, y)\n",
    "\n",
    "print(\"LinearSVC:                   \", lin_clf.intercept_, lin_clf.coef_)\n",
    "print(\"SVC:                         \", svm_clf.intercept_, svm_clf.coef_)\n",
    "print(\"SGDClassifier(alpha={:.5f}):\".format(sgd_clf.alpha), sgd_clf.intercept_, sgd_clf.coef_)\n",
    "LinearSVC:                    [0.28475098] [[1.05364854 1.09903804]]\n",
    "SVC:                          [0.31896852] [[1.1203284  1.02625193]]\n",
    "SGDClassifier(alpha=0.00200): [0.117] [[0.77714169 0.72981762]]\n",
    "Let's plot the decision boundaries of these three models:\n",
    "\n",
    "### Compute the slope and bias of each decision boundary\n",
    "w1 = -lin_clf.coef_[0, 0]/lin_clf.coef_[0, 1]\n",
    "b1 = -lin_clf.intercept_[0]/lin_clf.coef_[0, 1]\n",
    "w2 = -svm_clf.coef_[0, 0]/svm_clf.coef_[0, 1]\n",
    "b2 = -svm_clf.intercept_[0]/svm_clf.coef_[0, 1]\n",
    "w3 = -sgd_clf.coef_[0, 0]/sgd_clf.coef_[0, 1]\n",
    "b3 = -sgd_clf.intercept_[0]/sgd_clf.coef_[0, 1]\n",
    "\n",
    "### Transform the decision boundary lines back to the original scale\n",
    "line1 = scaler.inverse_transform([[-10, -10 * w1 + b1], [10, 10 * w1 + b1]])\n",
    "line2 = scaler.inverse_transform([[-10, -10 * w2 + b2], [10, 10 * w2 + b2]])\n",
    "line3 = scaler.inverse_transform([[-10, -10 * w3 + b3], [10, 10 * w3 + b3]])\n",
    "\n",
    "### Plot all three decision boundaries\n",
    "plt.figure(figsize=(11, 4))\n",
    "plt.plot(line1[:, 0], line1[:, 1], \"k:\", label=\"LinearSVC\")\n",
    "plt.plot(line2[:, 0], line2[:, 1], \"b--\", linewidth=2, label=\"SVC\")\n",
    "plt.plot(line3[:, 0], line3[:, 1], \"r-\", label=\"SGDClassifier\")\n",
    "plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\") # label=\"Iris versicolor\"\n",
    "plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\") # label=\"Iris setosa\"\n",
    "plt.xlabel(\"Petal length\", fontsize=14)\n",
    "plt.ylabel(\"Petal width\", fontsize=14)\n",
    "plt.legend(loc=\"upper center\", fontsize=14)\n",
    "plt.axis([0, 5.5, 0, 2])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69813d66",
   "metadata": {},
   "source": [
    "### 9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all 10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want to tune the hyperparameters using small validation sets. What level of precision can you achieve?\n",
    "\n",
    "**Ans.** First, let's load the dataset and split it into a training set and a test set. We could use train_test_split() but people usually just take the first 60,000 instances for the training set, and the last 10,000 instances for the test set (this makes it possible to compare your model's performance with others):\n",
    "\n",
    "Warning: since Scikit-Learn 0.24, fetch_openml() returns a Pandas DataFrame by default. To avoid this, we use as_frame=False.\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True, as_frame=False)\n",
    "\n",
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"].astype(np.uint8)\n",
    "\n",
    "X_train = X[:60000]\n",
    "y_train = y[:60000]\n",
    "X_test = X[60000:]\n",
    "y_test = y[60000:]\n",
    "Many training algorithms are sensitive to the order of the training instances, so it's generally good practice to shuffle them first. However, the dataset is already shuffled, so we do not need to do it.\n",
    "\n",
    "Let's start simple, with a linear SVM classifier. It will automatically use the One-vs-All (also called One-vs-the-Rest, OvR) strategy, so there's nothing special we need to do. Easy!\n",
    "\n",
    "Warning: this may take a few minutes depending on your hardware.\n",
    "\n",
    "lin_clf = LinearSVC(random_state=42)\n",
    "lin_clf.fit(X_train, y_train)\n",
    "/Users/ageron/miniconda3/envs/tf2/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "  \"the number of iterations.\", ConvergenceWarning)\n",
    "LinearSVC(random_state=42)\n",
    "Let's make predictions on the training set and measure the accuracy (we don't want to measure it on the test set yet, since we have not selected and trained the final model yet):\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = lin_clf.predict(X_train)\n",
    "accuracy_score(y_train, y_pred)\n",
    "0.8348666666666666\n",
    "Okay, 83.5% accuracy on MNIST is pretty bad. This linear model is certainly too simple for MNIST, but perhaps we just needed to scale the data first:\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(np.float32))\n",
    "X_test_scaled = scaler.transform(X_test.astype(np.float32))\n",
    "Warning: this may take a few minutes depending on your hardware.\n",
    "\n",
    "lin_clf = LinearSVC(random_state=42)\n",
    "lin_clf.fit(X_train_scaled, y_train)\n",
    "/Users/ageron/miniconda3/envs/tf2/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "  \"the number of iterations.\", ConvergenceWarning)\n",
    "LinearSVC(random_state=42)\n",
    "y_pred = lin_clf.predict(X_train_scaled)\n",
    "accuracy_score(y_train, y_pred)\n",
    "0.9217333333333333\n",
    "That's much better (we cut the error rate by about 53%), but still not great at all for MNIST. If we want to use an SVM, we will have to use a kernel. Let's try an SVC with an RBF kernel (the default).\n",
    "\n",
    "Note: to be future-proof we set gamma=\"scale\" since it will be the default value in Scikit-Learn 0.22.\n",
    "\n",
    "svm_clf = SVC(gamma=\"scale\")\n",
    "svm_clf.fit(X_train_scaled[:10000], y_train[:10000])\n",
    "SVC()\n",
    "y_pred = svm_clf.predict(X_train_scaled)\n",
    "accuracy_score(y_train, y_pred)\n",
    "0.9455333333333333\n",
    "That's promising, we get better performance even though we trained the model on 6 times less data. Let's tune the hyperparameters by doing a randomized search with cross validation. We will do this on a small dataset just to speed up the process:\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "param_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\n",
    "rnd_search_cv = RandomizedSearchCV(svm_clf, param_distributions, n_iter=10, verbose=2, cv=3)\n",
    "rnd_search_cv.fit(X_train_scaled[:1000], y_train[:1000])\n",
    "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
    "[CV] C=5.847490967837556, gamma=0.004375955271336425 .................\n",
    "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
    "[CV] .. C=5.847490967837556, gamma=0.004375955271336425, total=   0.8s\n",
    "[CV] C=5.847490967837556, gamma=0.004375955271336425 .................\n",
    "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n",
    "[CV] .. C=5.847490967837556, gamma=0.004375955271336425, total=   0.8s\n",
    "[CV] C=5.847490967837556, gamma=0.004375955271336425 .................\n",
    "[CV] .. C=5.847490967837556, gamma=0.004375955271336425, total=   0.8s\n",
    "[CV] C=2.544266730893301, gamma=0.024987648190235304 .................\n",
    "[CV] .. C=2.544266730893301, gamma=0.024987648190235304, total=   0.9s\n",
    "[CV] C=2.544266730893301, gamma=0.024987648190235304 .................\n",
    "[CV] .. C=2.544266730893301, gamma=0.024987648190235304, total=   0.9s\n",
    "[CV] C=2.544266730893301, gamma=0.024987648190235304 .................\n",
    "[CV] .. C=2.544266730893301, gamma=0.024987648190235304, total=   0.9s\n",
    "rnd_search_cv.best_estimator_\n",
    "SVC(C=3.8786881587000437, gamma=0.0017076019229344522)\n",
    "rnd_search_cv.best_score_\n",
    "0.8599947252641863\n",
    "This looks pretty low but remember we only trained the model on 1,000 instances. Let's retrain the best estimator on the whole training set:\n",
    "\n",
    "Warning: the following cell may take hours to run, depending on your hardware.\n",
    "\n",
    "rnd_search_cv.best_estimator_.fit(X_train_scaled, y_train)\n",
    "SVC(C=3.8786881587000437, gamma=0.0017076019229344522)\n",
    "y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)\n",
    "accuracy_score(y_train, y_pred)\n",
    "0.9978166666666667\n",
    "Ah, this looks good! Let's select this model. Now we can test it on the test set:\n",
    "\n",
    "y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)\n",
    "accuracy_score(y_test, y_pred)\n",
    "0.9717\n",
    "Not too bad, but apparently the model is overfitting slightly. It's tempting to tweak the hyperparameters a bit more (e.g. decreasing C and/or gamma), but we would run the risk of overfitting the test set. Other people have found that the hyperparameters C=5 and gamma=0.005 yield even better performance (over 98% accuracy). By running the randomized search for longer and on a larger part of the training set, you may be able to find this as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ffaa5",
   "metadata": {},
   "source": [
    "### 10. On the California housing dataset, train an SVM regressor.\n",
    "\n",
    "**Ans.** Let's load the dataset using Scikit-Learn's fetch_california_housing() function:\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X = housing[\"data\"]\n",
    "y = housing[\"target\"]\n",
    "Split it into a training set and a test set:\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "Don't forget to scale the data:\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "Let's train a simple LinearSVR first:\n",
    "\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "lin_svr = LinearSVR(random_state=42)\n",
    "lin_svr.fit(X_train_scaled, y_train)\n",
    "/Users/ageron/miniconda3/envs/tf2/lib/python3.7/site-packages/sklearn/svm/_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
    "  \"the number of iterations.\", ConvergenceWarning)\n",
    "LinearSVR(random_state=42)\n",
    "Let's see how it performs on the training set:\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = lin_svr.predict(X_train_scaled)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "mse\n",
    "0.9641780189948642\n",
    "Let's look at the RMSE:\n",
    "\n",
    "np.sqrt(mse)\n",
    "0.9819256687727764\n",
    "In this training set, the targets are tens of thousands of dollars. The RMSE gives a rough idea of the kind of error you should expect (with a higher weight for large errors): so with this model we can expect errors somewhere around $10,000. Not great. Let's see if we can do better with an RBF Kernel. We will use randomized search with cross validation to find the appropriate hyperparameter values for C and gamma:\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal, uniform\n",
    "\n",
    "param_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\n",
    "rnd_search_cv = RandomizedSearchCV(SVR(), param_distributions, n_iter=10, verbose=2, cv=3, random_state=42)\n",
    "rnd_search_cv.fit(X_train_scaled, y_train)\n",
    "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
    "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
    "[CV] C=4.745401188473625, gamma=0.07969454818643928 ..................\n",
    "[CV] ... C=4.745401188473625, gamma=0.07969454818643928, total=   4.7s\n",
    "[CV] C=4.745401188473625, gamma=0.07969454818643928 ..................\n",
    "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.7s remaining:    0.0s\n",
    "[CV] ... C=4.745401188473625, gamma=0.07969454818643928, total=   4.6s\n",
    "[CV] C=4.745401188473625, gamma=0.07969454818643928 ..................\n",
    "[CV] ... C=4.745401188473625, gamma=0.07969454818643928, total=   4.7s\n",
    "[CV] C=8.31993941811405, gamma=0.015751320499779724 ..................\n",
    "[CV] ... C=8.31993941811405, gamma=0.015751320499779724, total=   4.3s\n",
    "[CV] C=8.31993941811405, gamma=0.015751320499779724 ..................\n",
    "[CV] ... C=8.31993941811405, gamma=0.015751320499779724, total=   4.2s\n",
    "rnd_search_cv.best_estimator_\n",
    "SVR(C=4.745401188473625, gamma=0.07969454818643928)\n",
    "Now let's measure the RMSE on the training set:\n",
    "\n",
    "y_pred = rnd_search_cv.best_estimator_.predict(X_train_scaled)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "np.sqrt(mse)\n",
    "0.5727524770785359\n",
    "Looks much better than the linear model. Let's select this model and evaluate it on the test set:\n",
    "\n",
    "y_pred = rnd_search_cv.best_estimator_.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "np.sqrt(mse)\n",
    "0.5929168385528734.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
