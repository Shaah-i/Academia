{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment_11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. Write the Python code to implement a single neuron.\n",
    "\n",
    "**Ans.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random weights when training has started\n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "Displaying new weights after training\n",
      "[[1.95024663]\n",
      " [1.96483841]\n",
      " [1.96038419]]\n",
      "Testing network on new examples ->\n",
      "[0.96033857]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot, tanh\n",
    "class my_network():\n",
    "    \n",
    "    def __init__(self):\n",
    "        random.seed(1)\n",
    "        # 3x1 Weight matrix\n",
    "        self.weight_matrix = 2 * random.random((3, 1)) - 1\n",
    "    \n",
    "    def my_tanh(self, x):\n",
    "        return tanh(x)\n",
    "    \n",
    "    def my_tanh_derivative(self, x):\n",
    "        return 1.0 - tanh(x) ** 2\n",
    "    \n",
    "    # forward propagation\n",
    "    def my_forward_propagation(self, inputs):\n",
    "        return self.my_tanh(dot(inputs, self.weight_matrix))\n",
    "    \n",
    "    # training the neural network.\n",
    "    def train(self, train_inputs, train_outputs,num_train_iterations):\n",
    "        for iteration in range(num_train_iterations):\n",
    "            output = self.my_forward_propagation(train_inputs)\n",
    "            # Calculate the error in the output.\n",
    "            error = train_outputs - output\n",
    "        adjustment = dot(train_inputs.T, error *self.my_tanh_derivative(output))\n",
    "          # Adjust the weight matrix\n",
    "        self.weight_matrix += adjustment\n",
    "          \n",
    "# Driver Code\n",
    "if __name__ == \"__main__\":\n",
    "    my_neural = my_network()\n",
    "    print ('Random weights when training has started')\n",
    "    print (my_neural.weight_matrix)\n",
    "    train_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "    train_outputs = array([[0, 1, 1, 0]]).T\n",
    "    my_neural.train(train_inputs, train_outputs, 10000)\n",
    "    print ('Displaying new weights after training')\n",
    "    print (my_neural.weight_matrix)\n",
    "    \n",
    "    # Test the neural network with a new situation.\n",
    "    print (\"Testing network on new examples ->\")\n",
    "    print (my_neural.my_forward_propagation(array([1, 0, 0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. Write the Python code to implement ReLU.\n",
    "\n",
    "**Ans.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Relu on (1.0) gives 1.0\n",
      "Applying Relu on (-10.0) gives 0.0\n",
      "Applying Relu on (0.0) gives 0.0\n",
      "Applying Relu on (15.0) gives 15.0\n",
      "Applying Relu on (-20.0) gives 0.0\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return max(0.0, x)\n",
    "\n",
    "x = 1.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = -10.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = 0.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = 15.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))\n",
    "x = -20.0\n",
    "print('Applying Relu on (%.1f) gives %.1f' % (x, relu(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. Write the Python code for a dense layer in terms of matrix multiplication.\n",
    "\n",
    "**Ans.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "## using random numbers generator,\n",
    "np.random.seed(0)\n",
    "\n",
    "# define our dataset \n",
    "\n",
    "X = [[1, 2, 3, 2.5],\n",
    "     [2.0, 5.0, -1.0, 2.0],\n",
    "     [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "#define dense layer class\n",
    "\n",
    "class Dense_layer:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        \"\"\"\n",
    "        2 arguments: number of inputs and numbers of neurons\n",
    "        generate weight randomly and multiply with 0.1 to make the numbers smaller (between 0, 1)\n",
    "        generate bias  \n",
    "        \"\"\"\n",
    "        self.weight = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.bias = np.zeros((1, n_neurons))\n",
    "    \n",
    "    # define the forward function, it takes only 1 arrg : input (the dataset)\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. Write the Python code for a dense layer in plain Python (that is, with list comprehensions and functionality built into Python).\n",
    "\n",
    "**Ans.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter input 1: 22\n",
      "Enter input 2: 55\n",
      "The predicted output is: [68.64912824528332, 70.53808420560202, 45.51663263913876]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = [[random.random() for _ in range(output_size)] for _ in range(input_size)]\n",
    "        self.bias = [random.random() for _ in range(output_size)]\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        z = [sum([inputs[i] * self.weights[i][j] for i in range(len(inputs))]) + self.bias[j] for j in range(len(self.bias))]\n",
    "        return z\n",
    "    \n",
    "# create a dense layer with 2 inputs and 3 outputs\n",
    "layer = DenseLayer(2, 3)\n",
    "\n",
    "# get input values from the user\n",
    "input1 = float(input(\"Enter input 1: \"))\n",
    "input2 = float(input(\"Enter input 2: \"))\n",
    "\n",
    "# make a prediction based on the inputs\n",
    "inputs = [input1, input2]\n",
    "output = layer.forward(inputs)\n",
    "\n",
    "print(\"The predicted output is:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5. What is the “hidden size” of a layer?\n",
    "\n",
    "**Ans.** The number of hidden neurons should be between the size of the input layer and the size of the output layer. The number of hidden neurons should be 2/3 the size of the input layer, plus the size of the output layer. The number of hidden neurons should be less than twice the size of the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6. What does the t method do in PyTorch?\n",
    "\n",
    "**Ans.** Returns a view of this tensor with its dimensions reversed.\n",
    "If n is the number of dimensions in x, x.T is equivalent to x.permute(n-1, n-2, ..., 0).\n",
    "The use of Tensor.T() on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider mT to transpose batches of matrices or x.permute(*torch.arange(x.ndim - 1, -1, -1)) to reverse the dimensions of a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7. Why is matrix multiplication written in plain Python very slow?\n",
    "\n",
    "**Ans.** Python is the most popular programming language for data-science-related tasks. And many of those tasks are computation-heavy ones. One of them is matrix multiplication – a basic linear algebra operation, used by almost all ML algorithms, for example, to compute activations of layers of a neural network based on activations.Python is a simple language with great tooling, enormous community, and a lot of packages available. But it is too slow for running computational-heavy algorithms, crucial for any data science task. However, there is a simple bypass for this limit. We can use libraries like NumPy, which provides a nice Python API for running code written in faster languages.\n",
    "We can have the performance of those languages, without their complexity. And we can still use an ecosystem of Python packages, tooling, and community. In the case of using Python for data science, we not only shouldn’t reinvent the wheel – we mustn’t do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8. In matmul, why is ac==br?\n",
    "\n",
    "**Ans.** If there is mismatch in dimensions, and one vector is longer than the other, we can no longer carry out element wise multiplication.The dimensions of the resulting matrix will always be ar,bc. That is the number of rows comes from A and the number of columns comes from B.\n",
    "\n",
    "Here’s how it looks like in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:  \n",
      " [[15. 18. 21. 24.]\n",
      " [30. 36. 42. 48.]\n",
      " [45. 54. 63. 72.]\n",
      " [60. 72. 84. 96.]]\n",
      "Shape:  (4, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def matmul1(a,b):\n",
    "    ar,ac = a.shape\n",
    "    br,bc = b.shape\n",
    "    assert ac==br\n",
    "    p=(ar,bc)\n",
    "    c = np.zeros(p)\n",
    "    for i in range(ar):\n",
    "        for j in range(bc):\n",
    "            for k in range(ac): #or br\n",
    "                c[i,j] += a[i,k]*b[k,j]\n",
    "    return c\n",
    "mat1=np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]])\n",
    "mat2 = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "\n",
    "result = matmul1(mat1, mat2)\n",
    "print('Result: ', '\\n', result)\n",
    "print('Shape: ', result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9. In Jupyter Notebook, how do you measure the time taken for a single cell to execute?\n",
    "\n",
    "**Ans.** \n",
    "In Jupyter Notebook (IPython), you can use the magic commands `%timeit` and `%%timeit` to measure the execution time of your code. No need to import the timeit module.\n",
    "`%timeit`\n",
    "For `%timeit`, specify the target code after `%timeit` with a space.\n",
    "\n",
    "By default, number and repeat in `timeit.timeit()` are set automatically. It can also be specified with `-n` and `-r` options.\n",
    "\n",
    "The mean and standard deviation are calculated.\n",
    "\n",
    "`%%timeit`\n",
    "You can use the magic command `%%timeit` to measure the execution time of the cell.\n",
    "\n",
    "As an example, try executing the same process using NumPy. As with `%timeit`, `-n` and `-r` are optional.\n",
    "\n",
    "Note that `%%timeit` measures the execution time of the entire cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10. What is elementwise arithmetic?\n",
    "\n",
    "**Ans.** An element-wise operation is an operation between two tensors that operates on corresponding elements within the respective tensors. An element-wise operation operates on corresponding elements between tensors. Two elements are said to be corresponding if the two elements occupy the same position within the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q11. Write the PyTorch code to test whether every element of a is greater than the corresponding element of b.\n",
    "\n",
    "**Ans.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m      4\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m])\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([0, 2, 2])\n",
    "\n",
    "result = torch.gt(a, b)\n",
    "\n",
    "if torch.all(result):\n",
    "    print(\"All elements of a are greater than b\")\n",
    "else:\n",
    "    print(\"Not all elements of a are greater than b\")\n",
    "    print(\"Indices where the condition fails: \", torch.where(result == False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q12. What is a rank-0 tensor? How do you convert it to a plain Python data type?\n",
    "\n",
    "**Ans.** A tensor with rank 0 is a zero-dimensional array. The element of a zero-dimensional array is a point. This is represented as a Scalar in Math and has magnitude. Eg: s = 48.3. Shape - [].you can cast the datatype of a tensor to a new datatype by using the tf. cast function.\n",
    "We can access the data type of a tensor using the \". dtype\" attribute of the tensor. It returns the data type of the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q13. How does elementwise arithmetic help us speed up matmul?\n",
    "\n",
    "**Ans.** \n",
    "**elementwise** (not comparable) (mathematics). Obtained by operating on one element (of a matrix etc) at a time.\n",
    "The element wise multiplication operator (#) computes a new matrix with elements that are the products of the corresponding elements of matrix1 and matrix2. In addition to multiplying matrices that have the same dimensions, you can use the element wise multiplication operator to multiply a matrix and a scalar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q14. What are the broadcasting rules?\n",
    "\n",
    "**Ans.** \n",
    "When operating on two tensors, PyTorch compares their shapes elementwise. It starts with the trailing dimensions and works its way backward, adding 1 when it meets empty dimensions. Two dimensions are compatible when one of the following is true:\n",
    "\n",
    "They are equal. One of them is 1, in which case that dimension is broadcast to make it the same as the other.\n",
    "Arrays do not need to have the same number of dimensions. For example, if you have a 256×256×3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with three values. \n",
    "\n",
    "Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible:</br>\n",
    "Image  (3d tensor): 256 x 256 x 3</br>\n",
    "Scale  (1d tensor):  (1)   (1)  3</br>\n",
    "Result (3d tensor): 256 x 256 x 3</br>\n",
    "\n",
    "However, a 2D tensor of size 256×256 isn't compatible with our image:</br>\n",
    "Image  (3d tensor): 256 x 256 x   3</br>\n",
    "Scale  (2d tensor):  (1)  256 x 256</br>\n",
    "Error</br>\n",
    "\n",
    "In our earlier examples we had with a 3×3 matrix and a vector of size 3, broadcasting was done on the rows:</br>\n",
    "Matrix (2d tensor):   3 x 3</br>\n",
    "Vector (1d tensor): (1)   3</br>\n",
    "Result (2d tensor):   3 x 3</br>\n",
    "\n",
    "As an exercise, try to determine what dimensions to add (and where) when you need to normalize a batch of images of size 64 x 3 x 256 x 256 with vectors of three elements (one for the mean and one for the standard deviation).\n",
    "\n",
    "Another useful way of simplifying tensor manipulations is the use of Einstein summations convention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q15. What is expand_as? Show an example of how it can be used to match the results of broadcasting.\n",
    "\n",
    "**Ans.** \n",
    "Here the elements of c are expanded to make three rows that match, making the operation possible. Again, PyTorch doesn't actually create three copies of c in memory. This is done by the expand_as method behind the scenes:</br>\n",
    "`c.expand_as(m)\n",
    "tensor([[10., 20., 30.],\n",
    "        [10., 20., 30.],\n",
    "        [10., 20., 30.]])`\n",
    "        \n",
    "If we look at the corresponding tensor, we can ask for its storage property (which shows the actual contents of the memory used for the tensor) to check there is no useless data stored:</br>\n",
    "`t = c.expand_as(m)\n",
    "t.storage()\n",
    " 10.0\n",
    " 20.0\n",
    " 30.0\n",
    "[torch.FloatStorage of size 3]`\n",
    "\n",
    "Even though the tensor officially has nine elements, only three scalars are stored in memory. This is possible thanks to the clever trick of giving that dimension a stride of 0 (which means that when PyTorch looks for the next row by adding the stride, it doesn't move):</br>\n",
    "\n",
    "`t.stride(), t.shape\n",
    "((0, 1), torch.Size([3, 3]))`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.9 (tensorflowGPU)",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
