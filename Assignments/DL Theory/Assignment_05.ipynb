{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment_05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. Why would you want to use the Data API?\n",
    "\n",
    "**Ans.** APIs(Application programming interface)  are helpful because they allow you to make your data public so that anyone can use your company's software or analyze the data in order to get insights or develop solutions that would not have been possible without the API.\n",
    "\n",
    "1. `Easy integration`: The Data API provides a standardized way to integrate with different data sources and applications, making it easier to work with data from multiple sources.\n",
    "\n",
    "2. `Data consistency`: The Data API ensures that data is consistent across different applications and services, as it provides a common interface for accessing and manipulating data.\n",
    "\n",
    "3. `Security`: The Data API can provide secure access to data by allowing developers to control access to data sources and apply data-level security rules.\n",
    "\n",
    "4. `Automation`: The Data API can be used to automate data-related tasks, such as importing data from different sources, transforming data, and exporting data to different formats.\n",
    "\n",
    "5. `Scalability`: The Data API can be used to handle large amounts of data and can be scaled to accommodate growing data needs.\n",
    "\n",
    "Overall, the Data API can simplify the process of working with data and improve the efficiency and effectiveness of data-related tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. What are the benefits of splitting a large dataset into multiple files?\n",
    "\n",
    "**Ans.** \n",
    "1. Multiple Users can Access Data Simultaneously. When the data in a database is split in frontend and backend it can be easily supplied to multiple users sharing network. \n",
    "2. Provides Better Protection. \n",
    "3. Allows for Future Planning. \n",
    "4. Easy to Modify User Interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?\n",
    "\n",
    "**Ans.** You can use TensorBoard to visualize profiling data: if the GPU is not fully utilized then your input pipeline is likely to be the bottleneck.\n",
    "-You can fix it by making sure it reads and preprocesses the data in multiple threads in parallel, and ensuring it prefetches a few batches.\n",
    "-If this is insufficient to get your GPU to 100% usage during training, make sure your preprocessing code is optimized. -You can also try saving the dataset into multiple TFRecord files, and if necessary perform some of the preprocessing ahead of time so that it does not need to be done on the fly during training.\n",
    "-If necessary, use a machine with more CPU and RAM, and ensure that the GPU bandwidth is large enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. Can you save any binary data to a TFRecord file, or only serialized protocol buffers?\n",
    "\n",
    "**Ans.** \n",
    "Steps: \n",
    "1. Convert each observation into a tf.train.Feature acceptable format. A tf.train.Feature can accept three types, they are BytesList, Int64List, and FloatList.\n",
    "2. Map the features and create a feature message using tf.train.Example. ...\n",
    "3. Serialize the tf.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5. Why would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?\n",
    "\n",
    "**Ans.**  \n",
    "- The Example protobuf format has the advantage that TensorFlow provides some operations to parse it (the tf.io.parse*example() functions) without you having to define your own format. It is sufficiently flexible to represent instances in most datasets.\n",
    "- if it does not cover your use case, you can define your own protocol buffer, compile it using protoc (setting the --descriptor_set_out and --include_imports arguments to export the protobuf descriptor), and use the tf.io.decode_proto() function to parse the serialized protobufs (see the “Custom protobuf” section of the notebook for an example). It’s more complicated and it requires deploying the descriptor along with the model but it can be done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6. When using TFRecords, when would you want to activate compression? Why not do it systematically?\n",
    "\n",
    "**Ans.** When using TFRecords you will generally want to activate compression if the TFRecord files will need to be downloaded by the training script, as compression will make files smaller and thus reduce download time.\n",
    "-But if the files are located on the same machine as the training script, it’s usually preferable to leave compression off, to avoid wasting CPU for decompression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7. Data can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?\n",
    "\n",
    "**Ans.** \n",
    "* Pros:\n",
    "    - If you preprocess the data when creating the data files, the training script will run faster, since it will not have to perform preprocessing on the fly.\n",
    "    - In some cases, the preprocessed data will also be much smaller than the original data, so you can save some space and speed up downloads.\n",
    "    - It may also be helpful to materialize the preprocessed data, for example to inspect it or archive it.\n",
    "* Cons:\n",
    "    - It’s not easy to experiment with various preprocessing logics if you need to generate a preprocessed dataset for each variant.\n",
    "    - if you want to perform data augmentation, you have to materialize many variants of your dataset, which will use a large amount of disk space and take a lot of time to generate. the trained model will expect preprocessed data, so you will have to add preprocessing code in your application before it calls the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
