{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment_12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. How does unsqueeze help us to solve certain broadcasting problems?\n",
    "\n",
    "**Ans.** \n",
    "\n",
    "The unsqueeze function is used in PyTorch to add an extra dimension to a tensor. This can help us solve certain broadcasting problems where the tensor shapes do not match.\n",
    "\n",
    "Broadcasting is a technique in PyTorch that allows us to perform element-wise operations on tensors with different shapes. However, to do this, PyTorch requires that the shapes of the tensors are compatible. Two tensors are compatible for broadcasting if their shapes are equal, or if one of the tensors has a dimension of size 1.\n",
    "\n",
    "If we have two tensors that are not compatible for broadcasting, we can use unsqueeze to add an extra dimension to one of the tensors, effectively making it compatible with the other tensor. For example, consider the following tensors:\n",
    "\n",
    "`A = torch.tensor([1, 2, 3])`\n",
    "\n",
    "`B = torch.tensor([[4, 5, 6],`\n",
    "\n",
    " `[7, 8, 9]])`\n",
    "\n",
    "\n",
    "If we want to add A to every row of B, we need to reshape A to be a 2D tensor of shape (3, 1), so that it has the same number of columns as B. We can do this using the unsqueeze function:\n",
    "\n",
    "`A = A.unsqueeze(1)`\n",
    "\n",
    "`C = A + B`\n",
    "\n",
    "\n",
    "Now, A has shape (3, 1) and B has shape (2, 3), which are compatible for broadcasting. PyTorch automatically broadcasts A to have the same shape as B by copying its elements along the new dimension.\n",
    "\n",
    "In general, unsqueeze is a useful function when we need to add an extra dimension to a tensor to make it compatible with another tensor for broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. How can we use indexing to do the same operation as unsqueeze?\n",
    "\n",
    "**Ans.** \n",
    "We can use indexing to achieve the same operation as unsqueeze by creating a new axis or dimension in the tensor. In PyTorch, we can use the None keyword to create a new axis.\n",
    "\n",
    "For example, consider the following tensor:\n",
    "\n",
    "`A = torch.tensor([1, 2, 3])`\n",
    "\n",
    "To add a new axis to A using indexing, we can write:\n",
    "\n",
    "\n",
    "`A[:, None]`\n",
    "\n",
    "This creates a new axis at position 1, effectively reshaping A from a 1D tensor of shape (3,) to a 2D tensor of shape (3, 1). We can then use this reshaped tensor for broadcasting with another tensor, just as we did with unsqueeze.\n",
    "\n",
    "Similarly, to add a new axis to a 2D tensor like B from the previous example:\n",
    "\n",
    "`B = torch.tensor([[4, 5, 6],`\n",
    "                  `[7, 8, 9]])`\n",
    "\n",
    "We can use indexing as follows:\n",
    "\n",
    "\n",
    "`B[:, :, None]`\n",
    "\n",
    "\n",
    "This creates a new axis at position 2, effectively reshaping B from a 2D tensor of shape (2, 3) to a 3D tensor of shape (2, 3, 1). Again, we can use this reshaped tensor for broadcasting with another tensor.\n",
    "\n",
    "In general, using indexing to add new axes or dimensions can be a useful alternative to unsqueeze for reshaping tensors and making them compatible for broadcasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. How do we show the actual contents of the memory used for a tensor?\n",
    "\n",
    "**Ans.** The commonly used way to store such data is in a single array that is laid out as a single, contiguous block within memory. More concretely, a 3x3x3 tensor would be stored simply as a single array of 27 values, one after the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added to each row or each column of the matrix? (Be sure to check your answer by running this code in a notebook.)\n",
    "\n",
    "\n",
    "**Ans.** \n",
    "When adding a vector of size 3 to a matrix of size 3x3, the elements of the vector are added to each row of the matrix. This is because broadcasting in PyTorch operates along the last dimensions of the tensors.\n",
    "\n",
    "For example, consider the following code:\n",
    "\n",
    "import torch\n",
    "\n",
    "A = torch.ones((3, 3))\n",
    "\n",
    "B = torch.tensor([1, 2, 3])\n",
    "\n",
    "C = A + B\n",
    "\n",
    "print(C)\n",
    "\n",
    "This will output the following tensor:\n",
    "\n",
    "tensor([[2., 3., 4.],\n",
    "\n",
    "    `[2., 3., 4.],`\n",
    "    `[2., 3., 4.]])`\n",
    "As we can see, the values of B have been added to each row of A. This is because the last dimension of B (size 3) is broadcasted to match the last dimension of A (also size 3), and the addition operation is applied element-wise along this dimension.\n",
    "\n",
    "If we want to add the vector to each column of the matrix instead, we need to reshape the vector to be a column vector of size (3, 1) so that it matches the shape of the matrix along the last dimension. We can do this using the unsqueeze method or indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5. Do broadcasting and expand_as result in increased memory use? Why or why not?\n",
    "\n",
    "**Ans.** \n",
    "Here the elements of c are expanded to make three rows that match, making the operation possible. Again, PyTorch doesn't actually create three copies of c in memory. This is done by the expand_as method behind the scenes:\n",
    "`c.expand_as(m)\n",
    "tensor([[10., 20., 30.],\n",
    "        [10., 20., 30.],\n",
    "        [10., 20., 30.]])`\n",
    "        \n",
    "If we look at the corresponding tensor, we can ask for its storage property (which shows the actual contents of the memory used for the tensor) to check there is no useless data stored:\n",
    "`t = c.expand_as(m)\n",
    "t.storage()\n",
    " 10.0\n",
    " 20.0\n",
    " 30.0\n",
    "[torch.FloatStorage of size 3]`\n",
    "\n",
    "Even though the tensor officially has nine elements, only three scalars are stored in memory. This is possible thanks to the clever trick of giving that dimension a stride of 0 (which means that when PyTorch looks for the next row by adding the stride, it doesn't move):\n",
    "`t.stride(), t.shape\n",
    "((0, 1), torch.Size([3, 3])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6. Implement matmul using Einstein summation.\n",
    "\n",
    "**Ans.** \n",
    "Before using the PyTorch operation @ or torch.matmul, there is one last way we can implement matrix multiplication: Einstein summation (einsum). This is a compact representation for combining products and sums in a general way. We write an equation like this:\n",
    "\n",
    "ik,kj -> ij\n",
    "The left hand side represents the operands dimensions, separated by commas. Here we have two tensors that each have two dimensions (i,k and k,j). The right hand side represents the result dimensions, so here we have a tensor with two dimensions i,j.\n",
    "\n",
    "The rules of Einstein summation notation are as follows:\n",
    "\n",
    "Repeated indices on the left side are implicitly summed over if they are not on the right side.\n",
    "Each index can appear at most twice on the left side.\n",
    "The unrepeated indices on the left side must appear on the right side.\n",
    "So in our example, since k is repeated, we sum over that index. In the end the formula represents the matrix obtained when we put in (i,j) the sum of all the coefficients (i,k) in the first tensor multiplied by the coefficients (k,j) in the second tensor... which is the matrix product! Here is how we can code this in PyTorch:\n",
    "\n",
    "def matmul(a,b): return torch.einsum('ik,kj->ij', a, b)\n",
    "Einstein summation is a very practical way of expressing operations involving indexing and sum of products. Note that you can have just one member on the left hand side. For instance, this:\n",
    "\n",
    "torch.einsum('ij->ji', a)\n",
    "returns the transpose of the matrix a. You can also have three or more members. This:\n",
    "\n",
    "torch.einsum('bi,ij,bj->b', a, b, c)\n",
    "will return a vector of size b where the k-th coordinate is the sum of a[k,i] b[i,j] c[k,j]. This notation is particularly convenient when you have more dimensions because of batches. For example, if you have two batches of matrices and want to compute the matrix product per batch, you would could this:\n",
    "\n",
    "torch.einsum('bik,bkj->bij', a, b)\n",
    "Let's go back to our new matmul implementation using einsum and look at its speed:\n",
    "\n",
    "%timeit -n 20 t5 = matmul(m1,m2)\n",
    "68.7 µs ± 4.06 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n",
    "As you can see, not only is it practical, but it's very fast. einsum is often the fastest way to do custom operations in PyTorch, without diving into C++ and CUDA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7. What does a repeated index letter represent on the lefthand side of einsum?\n",
    "\n",
    "**Ans.** \n",
    "Before using the PyTorch operation @ or torch.matmul, there is one last way we can implement matrix multiplication: Einstein summation (einsum). This is a compact representation for combining products and sums in a general way. We write an equation like this:\n",
    "\n",
    "ik,kj -> ij\n",
    "The lefthand side represents the operands dimensions, separated by commas. Here we have two tensors that each have two dimensions (i,k and k,j). The righthand side represents the result dimensions, so here we have a tensor with two dimensions i,j.\n",
    "\n",
    "The rules of Einstein summation notation are as follows:\n",
    "\n",
    "Repeated indices on the left side are implicitly summed over if they are not on the right side.\n",
    "Each index can appear at most twice on the left side.\n",
    "The unrepeated indices on the left side must appear on the right side.\n",
    "So in our example, since k is repeated, we sum over that index. In the end the formula represents the matrix obtained when we put in (i,j) the sum of all the coefficients (i,k) in the first tensor multiplied by the coefficients (k,j) in the second tensor... which is the matrix product! Here is how we can code this in PyTorch:\n",
    "\n",
    "def matmul(a,b): return torch.einsum('ik,kj->ij', a, b)\n",
    "Einstein summation is a very practical way of expressing operations involving indexing and sum of products. Note that you can have just one member on the lefthand side. For instance, this:\n",
    "\n",
    "torch.einsum('ij->ji', a)\n",
    "returns the transpose of the matrix a. You can also have three or more members. This:\n",
    "\n",
    "torch.einsum('bi,ij,bj->b', a, b, c)\n",
    "will return a vector of size b where the k-th coordinate is the sum of a[k,i] b[i,j] c[k,j]. This notation is particularly convenient when you have more dimensions because of batches. For example, if you have two batches of matrices and want to compute the matrix product per batch, you would could this:\n",
    "\n",
    "torch.einsum('bik,bkj->bij', a, b)\n",
    "Let's go back to our new matmul implementation using einsum and look at its speed:\n",
    "\n",
    "%timeit -n 20 t5 = matmul(m1,m2)\n",
    "68.7 µs ± 4.06 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)\n",
    "As you can see, not only is it practical, but it's very fast. einsum is often the fastest way to do custom operations in PyTorch, without diving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8. What are the three rules of Einstein summation notation? Why?\n",
    "\n",
    "**Ans.** \n",
    "1. Each index can appear at most twice in any term.\n",
    "2. Repeated indices are implicitly summed over. \n",
    "3. Each term must contain identical non-repeated indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9. What are the forward pass and backward pass of a neural network?\n",
    "\n",
    "**Ans.** \n",
    "Backward and forward pass makes together one \"iteration\". During one iteration, you usually pass a subset of the data set, which is called \"mini-batch\" or \"batch\" , \"Epoch\" means passing the entire data set in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10. Why do we need to store some of the activations calculated for intermediate layers in the forward pass?\n",
    "\n",
    "**Ans.** Forward propagation refers to storage and calculation of input data which is fed in forward direction through the network to generate an output. Hidden layers in neural network accepts the data from the input layer, process it on the basis of activation function and pass it to the output layer or the successive layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q11. What is the downside of having activations with a standard deviation too far away from 1?\n",
    "\n",
    "**Ans.** \n",
    "Normal distribution with a mean of 0 and a standard deviation of 1 is called a standard normal distribution. Areas of the normal distribution are often represented by tables of the standard normal distribution. A portion of a table of the standard normal distribution .\n",
    "my standard deviation and variance are above 1, the standard deviation will be smaller than the variance. But if they are below 1, the standard deviation will be bigger than the variance.\n",
    "in a normal distribution, a score that is 1 s.d. above the mean is equivalent to the 84th percentile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q12. How can weight initialization help avoid this problem?\n",
    "\n",
    "**Ans.** The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
