{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment_06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. What are the advantages of a CNN over a fully connected DNN for image classification?\n",
    "\n",
    "**Ans.** Convolutional Neural Networks (CNNs) have several advantages over fully connected Deep Neural Networks (DNNs) for image classification. Here are some of the main advantages:\n",
    "\n",
    "1. Parameter efficiency: CNNs use shared weights and biases for the convolutional layers, which greatly reduces the number of parameters compared to fully connected DNNs. This makes CNNs more efficient for processing large images and reduces the risk of overfitting.\n",
    "\n",
    "2. Translation invariance: CNNs are designed to recognize patterns and features within an image regardless of their location, making them translation invariant. This is achieved through the use of convolutional layers that slide filters over the image to detect patterns in a local area. This property makes CNNs well-suited for image classification tasks where the object of interest can be anywhere in the image.\n",
    "\n",
    "3. Hierarchical feature learning: CNNs use multiple layers of feature extraction to learn hierarchical representations of images. This allows the network to learn high-level concepts, such as object categories, from low-level features, such as edges and corners. In contrast, fully connected DNNs treat all input features as independent and do not capture the spatial relationships between them.\n",
    "\n",
    "4. Robustness to input variations: CNNs are designed to be robust to variations in the input, such as changes in lighting, rotation, and scale. This is achieved through the use of techniques such as pooling, which reduces the sensitivity of the network to small changes in the input.\n",
    "\n",
    "Overall, the advantages of CNNs over fully connected DNNs make them the state-of-the-art approach for image classification tasks. CNN will outperform a fully-connected network if they have same number of hidden layers with same structure.Convolutional Neural Networks perform better than fully-connected networks on binary image classification, with a lot less parameters, because of their shared-weights architecture and translation invariance characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and &quot;same&quot; padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels. What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?\n",
    "\n",
    "**Ans.** \n",
    "**parameters:** \n",
    "First convolutional layer kernel-size and RGB channels, plus bias: 3 * 3 * 3 + 1 = 28 output feature maps is 100: 28 * 100 = 2800</br>\n",
    "Second convolutional layer kernel-size and last feature maps, plus bias: 3 * 3 * 100 + 1 = 901 output feature maps is 200: 901 * 200 = 180200</br>\n",
    "Third convolutional layers kernel-size and last feautre maps, plus bias: 3 * 3 * 200 + 1 =1801 output feautre maps is 400: 1801 * 400 = 720400</br>\n",
    "Total parameters is 2800 + 180200 + 720400 = 903400</br>\n",
    "\n",
    "Memories since 32-bit is 4 bytes</br>\n",
    "\n",
    "First convolutional layer one feature map size: 100 * 150 = 15000 total output: 15000 * 100 = 1,500,000</br>\n",
    "Second convolutional layer one feature map size: 50 * 75 = 3,750 total output: 3750 * 200 = 750,000</br>\n",
    "Third convolutional layer one feature map size: 25 * 38 = 950 total ouput: 950 * 400 = 380, 000</br>\n",
    "\n",
    "(1,500,000 + 750,000 + 380,000) * 4 / 1024 /1024 = 10.032 (MB) </br>\n",
    "903400 * 4 / 1024 / 1024 = 3.44 (MB)</br>\n",
    "\n",
    "Total memory is 10.032+ 3.44=13.47(MB)</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?\n",
    "\n",
    "**Ans.** \n",
    "1. Decrease batch size: One of the most effective ways to reduce memory usage is to decrease the batch size. This will reduce the number of samples being processed simultaneously, which in turn reduces the amount of memory required.\n",
    "\n",
    "2. Reduce input size: If the input images are very large, downsampling them to a smaller size can also reduce the memory requirements. However, this may come at the cost of reduced performance or accuracy.\n",
    "\n",
    "3. Use mixed precision training: Using mixed precision training, where the weights and activations are stored using 16-bit floating point numbers instead of 32-bit, can significantly reduce memory usage without sacrificing accuracy.\n",
    "\n",
    "4. Reduce model complexity: Another way to reduce memory usage is to reduce the complexity of the model. This can be done by reducing the number of layers, filters, or neurons in the network, or by using a smaller kernel size. (Remove one or more layers.)\n",
    "\n",
    "5. Use gradient checkpointing: Gradient checkpointing is a technique that trades off computation time for memory usage. Instead of storing all the intermediate activations during the forward pass for use during the backward pass, only a subset of the activations are stored. This reduces memory usage, but increases computation time during the backward pass.\n",
    "\n",
    "These are just some of the techniques that can be used to reduce memory usage during CNN training. The best approach will depend on the specific model, dataset, and hardware being used, and may require experimentation to find the optimal solution.\n",
    "\n",
    "Some other ways are:\n",
    "1. Reduce dimensionality using a larger stride in one or more layers.\n",
    "2. Use 16-bit floats instead of 32-bit floats.\n",
    "3. Distribute the CNN across multiple devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?\n",
    "\n",
    "**Ans.** Max pooling layers and convolutional layers with the same stride both reduce the spatial resolution of the feature maps. However, there are some reasons why one might want to use max pooling instead of convolution:\n",
    "\n",
    "Non-linearity: Max pooling introduces a non-linear activation function that can help the network learn more complex features. In contrast, a convolutional layer with the same stride only applies a linear transformation to the input.\n",
    "\n",
    "Translation invariance: Max pooling is more translation invariant than convolution with stride. This means that the features learned by the network are less sensitive to small shifts in the input image, which can improve the robustness of the network to small variations in the data.\n",
    "\n",
    "Computational efficiency: Max pooling is computationally more efficient than convolution with the same stride because it does not require any learnable parameters.\n",
    "\n",
    "Reduce overfitting: Max pooling can be used to reduce overfitting by reducing the spatial resolution of the feature maps and introducing some degree of spatial invariance.\n",
    "\n",
    "Overall, max pooling can be a useful tool for reducing the spatial resolution of feature maps while introducing non-linearity, translation invariance, and computational efficiency. However, the choice between max pooling and convolution with the same stride will depend on the specific requirements of the model and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5. When would you want to add a local response normalization layer?\n",
    "\n",
    "**Ans.** A local response normalization (LRN) layer can be added to a CNN to improve its generalization performance by promoting competition among feature maps. The main purpose of LRN is to normalize the response of a neuron by dividing it by the sum of squares of its neighboring activations.\n",
    "\n",
    "Here are some scenarios when one may want to add a local response normalization layer to a CNN:\n",
    "\n",
    "Large-scale datasets: LRN is more effective on large-scale datasets as it helps the network generalize better and reduce overfitting. When working with large-scale datasets, adding an LRN layer can improve the performance of the network.\n",
    "\n",
    "Convolutional layers with large receptive fields: When the receptive field of a convolutional layer is large, there is a high chance that some feature maps will respond strongly to stimuli that are irrelevant to the target object. In such cases, LRN can help the network suppress the irrelevant feature maps and promote the relevant ones.\n",
    "\n",
    "Networks with multiple convolutional layers: When working with CNNs with multiple convolutional layers, adding an LRN layer can help the network learn more robust features by encouraging competition among feature maps. This can help prevent the network from overfitting to the training data.\n",
    "\n",
    "Image classification tasks: LRN has been shown to be particularly effective in improving the performance of CNNs on image classification tasks. It can help the network learn more discriminative features, which in turn can improve its classification accuracy.\n",
    "\n",
    "Overall, adding a local response normalization layer to a CNN can be useful in scenarios where there is a need to improve the generalization performance of the network or promote competition among feature maps. However, the decision to add an LRN layer will depend on the specific requirements of the model and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6. Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?\n",
    "\n",
    "**Ans.** \n",
    "AlexNet:\n",
    "\n",
    "* AlexNet was one of the first successful deep convolutional neural networks for image classification.\n",
    "* It had 5 convolutional layers and 3 fully connected layers.\n",
    "* AlexNet used Rectified Linear Units (ReLU) activation function instead of the traditional sigmoid activation function used in LeNet-5.\n",
    "* It also used data augmentation techniques such as cropping and flipping to reduce overfitting and increase the size of the training dataset.\n",
    "* AlexNet used dropout regularization to further reduce overfitting.\n",
    "* AlexNet was trained on two GPUs simultaneously, which reduced the training time significantly.\n",
    "\n",
    "GoogLeNet:\n",
    "\n",
    "* GoogLeNet introduced the concept of Inception modules, which allowed for the efficient use of multiple filter sizes in a single layer.\n",
    "* It had 22 layers and was the first network to exceed human-level performance on the ImageNet classification task.\n",
    "* GoogLeNet also used global average pooling instead of fully connected layers, which significantly reduced the number of parameters in the network.\n",
    "* It used a 1x1 convolutional layer to reduce the dimensionality of the feature maps before applying more computationally expensive convolutional layers.\n",
    "\n",
    "ResNet:\n",
    "\n",
    "* ResNet introduced the concept of residual blocks, which allowed for the training of very deep networks without suffering from the problem of vanishing gradients.\n",
    "* It had up to 152 layers and achieved state-of-the-art performance on several computer vision tasks.\n",
    "* ResNet used skip connections to connect earlier layers directly to later layers, which helped to propagate gradients more effectively during training.\n",
    "* It also used batch normalization to accelerate training and improve generalization.\n",
    "\n",
    "SENet:\n",
    "\n",
    "* SENet introduced the concept of Squeeze-and-Excitation (SE) blocks, which allowed the network to adaptively recalibrate the importance of each feature map based on the global context of the image.\n",
    "* It achieved state-of-the-art performance on several computer vision tasks.\n",
    "* SENet used a global average pooling layer to reduce the dimensionality of the feature maps before applying the SE blocks.\n",
    "* It also used residual connections to help propagate gradients more effectively during training.\n",
    "\n",
    "Xception:\n",
    "\n",
    "* Xception introduced the concept of depthwise separable convolutions, which separate the spatial filtering and channel-wise filtering operations.\n",
    "* It had fewer parameters and was more computationally efficient than other state-of-the-art models while achieving comparable or better performance.\n",
    "* Xception used residual connections and batch normalization to accelerate training and improve generalization.\n",
    "* It also used global average pooling instead of fully connected layers to reduce the number of parameters in the network.\n",
    "\n",
    "These are just a few of the innovations introduced in each of these models. There have been many other contributions to the field of deep learning for computer vision, each building upon the successes of the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7. What is a fully convolutional network? How can you convert a dense layer into a convolutional layer?\n",
    "\n",
    "**Ans.**  FCN is a network that does not contain any “Dense” layers (as in traditional CNNs) instead it contains 1x1 convolutions that perform the task of fully connected layers (Dense layers).\n",
    "A fully convolution network can be built by simply replacing the FC layers with there equivalent Conv layers. In the example of VGG16 we can do so by first removing the last four layers. One way to do so is to pop layers from the model. In the model stack, each popping will remove the last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8. What is the main technical difficulty of semantic segmentation?\n",
    "\n",
    "**Ans.** Semantic Segmentation follows three steps:\n",
    "* Classifying: Classifying a certain object in the image.\n",
    "* Localizing: Finding the object and drawing a bounding box around it.\n",
    "* Segmentation: Grouping the pixels in a localized image by creating a segmentation mask.\n",
    "\n",
    "The task of Semantic Segmentation can be referred to as classifying a certain class of image and separating it from the rest of the image classes by overlaying it with a segmentation mask.It can also be thought of as the classification of images at a pixel level.The goal is simply to take an image and generate an output such that it contains a segmentation map where the pixel value (from 0 to 255) of the input image is transformed into a class label value (0, 1, 2, … n).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9. Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.\n",
    "\n",
    "**Ans.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 44s 22ms/step - loss: 0.2112 - accuracy: 0.9348 - val_loss: 0.0467 - val_accuracy: 0.9847\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 40s 22ms/step - loss: 0.0802 - accuracy: 0.9762 - val_loss: 0.0351 - val_accuracy: 0.9878\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 41s 22ms/step - loss: 0.0559 - accuracy: 0.9834 - val_loss: 0.0294 - val_accuracy: 0.9906\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 43s 23ms/step - loss: 0.0458 - accuracy: 0.9859 - val_loss: 0.0280 - val_accuracy: 0.9907\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 48s 26ms/step - loss: 0.0378 - accuracy: 0.9882 - val_loss: 0.0270 - val_accuracy: 0.9919\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 57s 30ms/step - loss: 0.0313 - accuracy: 0.9906 - val_loss: 0.0290 - val_accuracy: 0.9915\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0293 - accuracy: 0.9908 - val_loss: 0.0284 - val_accuracy: 0.9916\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.0233 - accuracy: 0.9927 - val_loss: 0.0270 - val_accuracy: 0.9917\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 49s 26ms/step - loss: 0.0218 - accuracy: 0.9935 - val_loss: 0.0277 - val_accuracy: 0.9922\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 50s 27ms/step - loss: 0.0210 - accuracy: 0.9934 - val_loss: 0.0233 - val_accuracy: 0.9929\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values to the range [0, 1]\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Add a channel dimension to the images\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 2s - loss: 0.0233 - accuracy: 0.9929 - 2s/epoch - 8ms/step\n",
      "\n",
      "Test accuracy: 0.992900013923645\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test,  y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10. Use transfer learning for large image classification, going through these steps:\n",
    "    a. Create a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow Datasets).\n",
    "    b. Split it into a training set, a validation set, and a test set.\n",
    "    c. Build the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation.\n",
    "    d. Fine-tune a pretrained model on this dataset.\n",
    "**Ans.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (23.0.1)\n",
      "Collecting install\n",
      "  Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\n",
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
      "     ---------------------------------------- 5.4/5.4 MB 5.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (2.27.1)\n",
      "Collecting dm-tree\n",
      "  Using cached dm_tree-0.1.8-cp310-cp310-win_amd64.whl (101 kB)\n",
      "Collecting protobuf>=3.20\n",
      "  Using cached protobuf-4.22.3-cp310-abi3-win_amd64.whl (420 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (1.14.1)\n",
      "Requirement already satisfied: termcolor in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (2.2.0)\n",
      "Collecting promise\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-1.13.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: toml in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (5.9.0)\n",
      "Requirement already satisfied: click in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (8.0.4)\n",
      "Collecting etils[enp,epath]>=0.9.0\n",
      "  Using cached etils-1.2.0-py3-none-any.whl (120 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (4.64.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (1.4.0)\n",
      "Collecting array-record\n",
      "  Using cached array_record-0.2.0-py310-none-any.whl (3.0 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (1.23.5)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (4.2.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (3.8.0)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (5.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->tensorflow-datasets) (0.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
      "     -------------------------------------- 223.6/223.6 kB 3.4 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21563 sha256=893f1f3cdb00f2a3cfa093eda25e552a3ba0df6fca29314f07cc55d7321da051\n",
      "  Stored in directory: c:\\users\\happysoul\\appdata\\local\\pip\\cache\\wheels\\54\\4e\\28\\3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, protobuf, promise, install, etils, googleapis-common-protos, tensorflow-metadata, array-record, tensorflow-datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\HappySoul\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\tree\\\\__init__.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-datasets\n",
      "  Using cached tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
      "Collecting protobuf>=3.20\n",
      "  Using cached protobuf-4.22.3-cp310-abi3-win_amd64.whl (420 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (1.23.5)\n",
      "Requirement already satisfied: wrapt in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (1.14.1)\n",
      "Requirement already satisfied: termcolor in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (2.2.0)\n",
      "Requirement already satisfied: click in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (8.0.4)\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-1.13.1-py3-none-any.whl (28 kB)\n",
      "Collecting dm-tree\n",
      "  Using cached dm_tree-0.1.8-cp310-cp310-win_amd64.whl (101 kB)\n",
      "Collecting etils[enp,epath]>=0.9.0\n",
      "  Using cached etils-1.2.0-py3-none-any.whl (120 kB)\n",
      "Collecting array-record\n",
      "  Using cached array_record-0.2.0-py310-none-any.whl (3.0 MB)\n",
      "Requirement already satisfied: psutil in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (5.9.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (4.64.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (1.4.0)\n",
      "Requirement already satisfied: toml in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Collecting promise\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-datasets) (2.27.1)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (5.9.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (3.8.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (4.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->tensorflow-datasets) (0.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\happysoul\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Using cached googleapis_common_protos-1.59.0-py2.py3-none-any.whl (223 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21563 sha256=e69e9c3a04460dbb07c781392ff1e4c4aa259cd532ff6b2abfb15a998ccfca30\n",
      "  Stored in directory: c:\\users\\happysoul\\appdata\\local\\pip\\cache\\wheels\\54\\4e\\28\\3ed0e1c8a752867445bab994d2340724928aa3ab059c57c8db\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, protobuf, promise, etils, googleapis-common-protos, tensorflow-metadata, array-record, tensorflow-datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\HappySoul\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\google\\\\~rotobuf\\\\internal\\\\_api_implementation.cp310-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Duplicate registrations for type 'experimentalOptimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, GlobalAveragePooling2D\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minception_v3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InceptionV3\n\u001b[0;32m      8\u001b[0m (train_ds, val_ds, test_ds), info \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcats_vs_dogs\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m                                              split\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain[:80\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain[80\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m:90\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain[90\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m:]\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     10\u001b[0m                                              with_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m                                              as_supervised\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\api\\_v2\\keras\\models\\__init__.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_from_json\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_from_yaml\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_model\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\saving\\saving_api.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save \u001b[38;5;28;01mas\u001b[39;00m legacy_sm_saving_lib\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io_utils\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\saving\\saving_lib.py:33\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizer\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ObjectSharingScope\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mserialization_lib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize_keras_object\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer.py:1293\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   1289\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;66;03m# Register the optimizer for loading from saved_model purpose.\u001b[39;00m\n\u001b[1;32m-> 1293\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_revived_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexperimentalOptimizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptimizer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaved_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVersionedTypeRegistration\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobject_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mRestoredOptimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m            \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_producer_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmin_consumer_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m   1306\u001b[0m Optimizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m Optimizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;241m.\u001b[39mreplace(\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mbase_optimizer_keyword_args}}\u001b[39m\u001b[38;5;124m\"\u001b[39m, base_optimizer_keyword_args\n\u001b[0;32m   1308\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\saved_model\\revived_types.py:133\u001b[0m, in \u001b[0;36mregister_revived_type\u001b[1;34m(identifier, predicate, versions)\u001b[0m\n\u001b[0;32m    130\u001b[0m   version_numbers\u001b[38;5;241m.\u001b[39madd(registration\u001b[38;5;241m.\u001b[39mversion)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m identifier \u001b[38;5;129;01min\u001b[39;00m _REVIVED_TYPE_REGISTRY:\n\u001b[1;32m--> 133\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate registrations for type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    135\u001b[0m _REVIVED_TYPE_REGISTRY[identifier] \u001b[38;5;241m=\u001b[39m (predicate, versions)\n\u001b[0;32m    136\u001b[0m _TYPE_IDENTIFIERS\u001b[38;5;241m.\u001b[39mappend(identifier)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Duplicate registrations for type 'experimentalOptimizer'"
     ]
    }
   ],
   "source": [
    "#  a\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "(train_ds, val_ds, test_ds), info = tfds.load('cats_vs_dogs',\n",
    "                                             split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "                                             with_info=True,\n",
    "                                             as_supervised=True)\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_crop(image, size=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    return image, label\n",
    "\n",
    "train_ds = train_ds.map(preprocess_image).shuffle(1000).batch(BATCH_SIZE)\n",
    "val_ds = val_ds.map(preprocess_image).batch(BATCH_SIZE)\n",
    "test_ds = test_ds.map(preprocess_image).batch(BATCH_SIZE)\n",
    "\n",
    "base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_ds, epochs=5, validation_data=val_ds)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_ds)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  b\n",
    "\n",
    "(train_ds, val_ds, test_ds), info = tfds.load('cats_vs_dogs',\n",
    "                                             split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "                                             with_info=True,\n",
    "                                             as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  c\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the dataset\n",
    "(train_ds, val_ds, test_ds), info = tfds.load('cats_vs_dogs',\n",
    "                                             split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "                                             with_info=True,\n",
    "                                             as_supervised=True)\n",
    "\n",
    "# Define preprocessing functions\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    # Resize the image to the input size of the model\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    # Convert the pixel values to the range [0, 1]\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "def augment_image(image, label):\n",
    "    # Randomly flip the image horizontally\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    # Randomly adjust the brightness of the image\n",
    "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "    return image, label\n",
    "\n",
    "# Create the training dataset\n",
    "train_ds = train_ds.shuffle(10000)\n",
    "train_ds = train_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.map(augment_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.batch(batch_size=32)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create the validation dataset\n",
    "val_ds = val_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(batch_size=32)\n",
    "val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Create the test dataset\n",
    "test_ds = test_ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.batch(batch_size=32)\n",
    "test_ds = test_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  d\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# Load the dataset\n",
    "(train_ds, val_ds, test_ds), info = tfds.load('cats_vs_dogs',\n",
    "                                             split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "                                             with_info=True,\n",
    "                                             as_supervised=True)\n",
    "\n",
    "# Define preprocessing functions\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    # Resize the image to the input size of the model\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    # Convert the pixel values to the range [-1, 1]\n",
    "    image = tf.cast(image, tf.float32) / 127.5 - 1.0\n",
    "    return image, label\n",
    "\n",
    "# Apply preprocessing to the datasets\n",
    "train_ds = train_ds.map(preprocess_image).shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.map(preprocess_image).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.map(preprocess_image).batch(32)\n",
    "\n",
    "# Load the pre-trained MobileNetV2 model\n",
    "base_model = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "# Freeze the base model's layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Add a new classification layer on top of the base model\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output_layer = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(global_average_layer)\n",
    "\n",
    "# Create the fine-tuned model\n",
    "model = tf.keras.models.Model(inputs=base_model.input, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training set\n",
    "history = model.fit(train_ds, epochs=10, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(f'Test loss: {loss}, Test accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
