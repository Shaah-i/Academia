{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment_03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "\n",
    "**Ans.** \n",
    "* No, all weights should be sampled independently; they should not all have the same initial value.\n",
    "* One important goal of sampling weights randomly is to break symmetry: if all the weights have the same initial value, even if that value is not zero, then symmetry is not broken  and backpropagation will be unable to break it.this means that all the neurons in any given layer will always have the same weights. It’s like having just one neuron per layer, and much slower.\n",
    "* It is virtually impossible for such a configuration to converge to a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "**Ans.** It is perfectly fine to initialize the bias terms to zero. Some people like to initialize them just like weights and that’s okay too it does not make much difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. Name three advantages of the SELU activation function over ReLU.\n",
    "\n",
    "**Ans.** \n",
    "* It can take on negative values, so the average output of the neurons in any given layer is typically closer to zero than when using the ReLU activation function This helps alleviate the vanishing gradients problem.\n",
    "* It always has a nonzero derivative, which avoids the dying units issue that can affect ReLU units.\n",
    "* When the conditions are right then the SELU activation function ensures the model is self-normalized, which solves the exploding/vanishing gradients problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "**Ans.** \n",
    "* The SELU activation function is a good default.\n",
    "* If you need the neural network to be as fast as possible, you can use one of the leaky ReLU variants instead (e.g., a simple leaky ReLU using the default hyperparameter value).\n",
    "* Simplicity of the ReLU activation function makes it many people’s preferred option, despite the fact that it is generally outperformed by SELU and leaky ReLU. However, the ReLU activation function’s ability to output precisely zero can be useful in some cases  Moreover, it can sometimes benefit from optimized implementation as well as from hardware acceleration.\n",
    "* Hyperbolic tangent (tanh) can be useful in the output layer if you need to output a number between –1 and 1, but nowadays it is not used much in hidden layers (except in recurrent nets).\n",
    "* Logistic activation function is also useful in the output layer when you need to estimate a probability rare in hidden layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "\n",
    "**Ans.** If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed,moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again and so on. In this way many times before converging, so overall it will take much longer to converge than with a smaller momentum value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6. Name three ways you can produce a sparse model.\n",
    "\n",
    "**Ans.** \n",
    "* One way to produce a sparse model (i.e., with most weights equal to zero) is to train the model normally, then zero out tiny weights.\n",
    "* For more sparsity, you can apply ℓ1 regularization during training, which pushes the optimizer toward sparsity.\n",
    "* A third option is to use the TensorFlow Model Optimization Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
    "\n",
    "**Ans.** One way to produce a sparse model (i.e., with most weights equal to zero) is to train the model normally, then zero out tiny weights.\n",
    "-For more sparsity, you can apply ℓ1 regularization during training, which pushes the optimizer toward sparsity.\n",
    "-A third option is to use the TensorFlow Model Optimization Toolkit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "\n",
    "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
    "\n",
    "b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "\n",
    "c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "\n",
    "d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "\n",
    "e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n",
    "\n",
    "**Ans.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1563/1563 [==============================] - 84s 35ms/step - loss: 2.0036 - accuracy: 0.2679 - val_loss: 1.8055 - val_accuracy: 0.3355\n",
      "Epoch 2/30\n",
      "1563/1563 [==============================] - 55s 35ms/step - loss: 1.8198 - accuracy: 0.3380 - val_loss: 1.7410 - val_accuracy: 0.3840\n",
      "Epoch 3/30\n",
      "1563/1563 [==============================] - 57s 37ms/step - loss: 1.7557 - accuracy: 0.3672 - val_loss: 1.7248 - val_accuracy: 0.3733\n",
      "Epoch 4/30\n",
      "1563/1563 [==============================] - 56s 36ms/step - loss: 1.7137 - accuracy: 0.3843 - val_loss: 1.6804 - val_accuracy: 0.4076\n",
      "Epoch 5/30\n",
      "1563/1563 [==============================] - 58s 37ms/step - loss: 1.6715 - accuracy: 0.4032 - val_loss: 1.6702 - val_accuracy: 0.4136\n",
      "Epoch 6/30\n",
      "1563/1563 [==============================] - 61s 39ms/step - loss: 1.6433 - accuracy: 0.4174 - val_loss: 1.6246 - val_accuracy: 0.4306\n",
      "Epoch 7/30\n",
      "1563/1563 [==============================] - 60s 38ms/step - loss: 1.6121 - accuracy: 0.4261 - val_loss: 1.5862 - val_accuracy: 0.4343\n",
      "Epoch 8/30\n",
      "1563/1563 [==============================] - 56s 36ms/step - loss: 1.5894 - accuracy: 0.4313 - val_loss: 1.5864 - val_accuracy: 0.4409\n",
      "Epoch 9/30\n",
      "1563/1563 [==============================] - 45s 28ms/step - loss: 1.5624 - accuracy: 0.4440 - val_loss: 1.6045 - val_accuracy: 0.4403\n",
      "Epoch 10/30\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 1.5444 - accuracy: 0.4516 - val_loss: 1.5714 - val_accuracy: 0.4468\n",
      "Epoch 11/30\n",
      "1563/1563 [==============================] - 38s 24ms/step - loss: 1.5198 - accuracy: 0.4599 - val_loss: 1.5341 - val_accuracy: 0.4461\n",
      "Epoch 12/30\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5046 - accuracy: 0.4680 - val_loss: 1.5596 - val_accuracy: 0.4492\n",
      "Epoch 13/30\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.4865 - accuracy: 0.4749 - val_loss: 1.5250 - val_accuracy: 0.4652\n",
      "Epoch 14/30\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.4704 - accuracy: 0.4806 - val_loss: 1.5328 - val_accuracy: 0.4560\n",
      "Epoch 15/30\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.4528 - accuracy: 0.4883 - val_loss: 1.5377 - val_accuracy: 0.4572\n",
      "Epoch 16/30\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 14.5525 - accuracy: 0.3553 - val_loss: 1.7630 - val_accuracy: 0.3506\n",
      "Epoch 17/30\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.6823 - accuracy: 0.3837 - val_loss: 1.7066 - val_accuracy: 0.3810\n",
      "Epoch 18/30\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 174.2887 - accuracy: 0.3268 - val_loss: 1.9907 - val_accuracy: 0.2297\n",
      "Epoch 19/30\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8907 - accuracy: 0.2951 - val_loss: 1.7909 - val_accuracy: 0.3466\n",
      "Epoch 20/30\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.7524 - accuracy: 0.3530 - val_loss: 1.7140 - val_accuracy: 0.3676\n",
      "Epoch 21/30\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.7083 - accuracy: 0.3718 - val_loss: 1.6954 - val_accuracy: 0.3767\n",
      "Epoch 22/30\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.6861 - accuracy: 0.3803 - val_loss: 1.6603 - val_accuracy: 0.3915\n",
      "Epoch 23/30\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.6652 - accuracy: 0.3887 - val_loss: 1.7512 - val_accuracy: 0.3551\n",
      "Epoch 24/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6514 - accuracy: 0.3959 - val_loss: 1.6996 - val_accuracy: 0.3812\n",
      "Epoch 25/30\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.6333 - accuracy: 0.4016 - val_loss: 1.7217 - val_accuracy: 0.3725\n",
      "Epoch 26/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6156 - accuracy: 0.4079 - val_loss: 1.6541 - val_accuracy: 0.3969\n",
      "Epoch 27/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6065 - accuracy: 0.4143 - val_loss: 1.6564 - val_accuracy: 0.3760\n",
      "Epoch 28/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6071 - accuracy: 0.4169 - val_loss: 1.6416 - val_accuracy: 0.4072\n",
      "Epoch 29/30\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.5932 - accuracy: 0.4187 - val_loss: 1.6108 - val_accuracy: 0.4204\n",
      "Epoch 30/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.5822 - accuracy: 0.4265 - val_loss: 1.6571 - val_accuracy: 0.3861\n"
     ]
    }
   ],
   "source": [
    "#  1\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Build the model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='he_normal', activation='elu'))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=30, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Convert pixel values to float and normalize\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.09809999912977219\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1407/1407 [==============================] - 81s 50ms/step - loss: 1.2783 - accuracy: 0.5531 - val_loss: 1.1951 - val_accuracy: 0.5988\n",
      "Epoch 2/50\n",
      "1407/1407 [==============================] - 59s 42ms/step - loss: 0.9604 - accuracy: 0.6634 - val_loss: 1.0896 - val_accuracy: 0.6200\n",
      "Epoch 3/50\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 0.8374 - accuracy: 0.7086 - val_loss: 1.1558 - val_accuracy: 0.6130\n",
      "Epoch 4/50\n",
      "1407/1407 [==============================] - 58s 41ms/step - loss: 0.7383 - accuracy: 0.7415 - val_loss: 0.9132 - val_accuracy: 0.6896\n",
      "Epoch 5/50\n",
      "1407/1407 [==============================] - 57s 41ms/step - loss: 0.6558 - accuracy: 0.7711 - val_loss: 1.0791 - val_accuracy: 0.6392\n",
      "Epoch 6/50\n",
      "1407/1407 [==============================] - 67s 48ms/step - loss: 0.5850 - accuracy: 0.7960 - val_loss: 1.1308 - val_accuracy: 0.6490\n",
      "Epoch 7/50\n",
      "1407/1407 [==============================] - 87s 62ms/step - loss: 0.5186 - accuracy: 0.8158 - val_loss: 1.2137 - val_accuracy: 0.6302\n"
     ]
    }
   ],
   "source": [
    "#  3\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Convert pixel values to float and normalize\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Define the neural network architecture with Batch Normalization\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history_bn = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with Batch Normalization: 0.6801999807357788\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss_bn, test_acc_bn = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test accuracy with Batch Normalization:\", test_acc_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEWCAYAAACEz/viAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA/XklEQVR4nO3dd1zV9f7A8debLaAIghMF3Ish4kizMstsWVmaTa1bZt3mbXnrd7vVra63vNXNbO+yaWVm2dacqaC4FyoKuAAB2TI+vz/OgY7IOIzDYbyfjwcPON/xOe8vB877fMb38xFjDEoppVo3F2cHoJRSyvk0GSillNJkoJRSSpOBUkopNBkopZRCk4FSSik0GagmTkSMiPS2/vyaiPzDnmPr8DzXichPdY1TqeZOk4FyKBH5QUSerGT7ZSJyRETc7C3LGDPTGPOvBogp1Jo4yp/bGDPfGDO+vmVX85xhIlIqIq866jmUqg9NBsrR3geuFxGpsP0GYL4xptgJMTnDjUAGcLWIeDbmE4uIa2M+n2qeNBkoR1sIdADGlG0QEX/gEuADERkuImtEJFNEDovIyyLiUVlBIvKeiDxl8/hB6zmHROTmCsdeLCIbReSEiCSJyOM2u5dbv2eKSI6InCEi00Vkpc35o0RkvYhkWb+Pstm3TET+JSKrRCRbRH4SkcCqfgHWRHgj8H9AEXBphf2XiUi8Nda9IjLBuj1ARN61Xl+GiCy0bj8lVus22+a090TkVRH5XkRygbE1/D4QkTNFZLX1dUiyPscwETlqm0xEZJKIbKrqWlXzpclAOZQxJh/4HMubYZkpwE5jzCagBLgPCATOAMYBd9RUrvUN8wHgfKAPcF6FQ3Ktz9keuBi4XUQut+47y/q9vTHG1xizpkLZAcB3wEtYEtnzwHci0sHmsGuBm4COgIc1lqqcCQQDn2L5XUyzea7hwAfAg9ZYzwISrbs/BLyBQdbneaGa56joWuBpoC2wkmp+HyISAiwB5gJBQBQQb4xZD6QDts1nN1jjVS2MJgPVGN4HrhIRL+vjG63bMMbEGWP+MMYUG2MSgdeBs+0ocwrwrjFmqzEmF3jcdqcxZpkxZosxptQYsxn4xM5ywfJmuccY86E1rk+AnZz6if5dY8xum2QXVU1504AlxpgM4GNggoh0tO77C/COMeZna6wpxpidItIFuBCYaYzJMMYUGWN+tzN+gG+MMausZRbU8Pu4FvjFGPOJ9XnSjTHx1n3vA9dDeZK8wHoNqoXRZKAczhizEkgDLheRXsBwrG8oItJXRBZbO5NPAM9gqSXUpCuQZPP4gO1OERkhIktFJFVEsoCZdpZbVvaBCtsOAN1sHh+x+TkP8K2sIBFpA0wG5gNYayEHsbwBA3QH9lZyanfguDWB1IXt76am30dVMQB8BFwqIj5YEvAKY8zhOsakmjBNBqqxfIClRnA98KMx5qh1+6tYPnX3Mca0Ax4BKnY2V+YwljexMj0q7P8YWAR0N8b4Aa/ZlFvTVL2HgJAK23oAKXbEVdEVQDvgFWvCO4IlqZQ1FSUBvSo5LwkIEJH2lezLxdJ8BICIdK7kmIrXWN3vo6oYMMakAGuASViaiD6s7DjV/GkyUI3lAyzt+rdibSKyagucAHJEpD9wu53lfQ5MF5GBIuIN/LPC/rZYPlkXWNvlr7XZlwqUAj2rKPt7oK+IXCsibiJyNTAQWGxnbLamAe8A4ViakqKA0UCkiIQDbwM3icg4EXERkW4i0t/66XsJliTiLyLuIlLW17EJGCQiUdamt8ftiKO638d84DwRmWK93g4iEmWz/wPgIes1fFWH34FqBjQZqEZh7Q9YDfhg+YRa5gEsb0zZwJvAZ3aWtwR4EfgNSLB+t3UH8KSIZAOPYUkeZefmYelcXWUdPTOyQtnpWEY73Y+lA/Uh4BJjTJo9sZURkW5YOsRfNMYcsfmKA34Aphlj1mHpiH4ByAJ+589ayQ1YRh/tBI4B91rj2w08CfwC7MHSQVyT6n4fB4GLrNd7HIgHIm3O/doa09fW351qgUQXt1FK1URE9gK3GWN+cXYsyjG0ZqCUqpaIXImlD6Ji7Uu1IHZPBaCUan1EZBmW/pIbjDGlTg5HOZA2EymllNJmIqWUUs2wmSgwMNCEhoY6OwyllGpW4uLi0owxQVXtb3bJIDQ0lNjYWGeHoZRSzYqIVLyr/hTaTKSUUsqxyUBEJojILhFJEJFZlex/wTp1b7yI7BaRTEfGo5RSqnIOayayzoE+D8sUw8nAehFZZIzZXnaMMeY+m+PvAoY4Kh6llFJVc2SfwXAgwRizD0BEPgUuA7ZXcfw1nD6/jFKNpqioiOTkZAoKCpwdilJ15uXlRXBwMO7u7rU6z5HJoBunTqObDIyo7EDr4hphVHGHo4jMAGYA9OhRcXJKpRpGcnIybdu2JTQ0FDltlU6lmj5jDOnp6SQnJxMWFlarc5tKB/JUYIExpqSyncaYN4wxMcaYmKCgKkdGKVUvBQUFdOjQQROBarZEhA4dOtSpduvIZJDCqfPNB1P1fPBTsay8pJRTaSJQzV1d/4YdmQzWA31EJEwsC5xP5dSpiwGwzmHvj2UBjZZr20I4vt/ZUSilVKUclgyMMcXAncCPwA7gc2PMNhF5UkQm2hw6FfjUtORJkrKPwBfT4dt7nB2JaqLuu+8+XnzxxfLHF1xwAbfcckv54/vvv5/nn3+eRYsWMXv2bAAWLlzI9u1/jsc455xzGuyGzGeeeabKfaGhoYSHhxMVFUV4eDjffPNNvcorM336dBYsWFDjcSLC/fffX/54zpw5PP744zWe15Bsf9cXXXQRmZmZtS7jxRdfJC/vz+Uh6lpOQ3Fon4Ex5ntjTF9jTC9jzNPWbY8ZYxbZHPO4Mea0exBalJ2LAQP7f4ekdc6ORjVBo0ePZvXq1QCUlpaSlpbGtm3byvevXr2aUaNGMXHiRGbNsvy7VEwGDammN++lS5cSHx/PggULuPvuu+tdXm14enry1VdfkZZWq7WGyhUXFzdYLADff/897du3r/V5FZNBXctpKE2lA7ll27EY/EOhTQAsn+PsaFQTNGrUKNassbSUbtu2jcGDB9O2bVsyMjIoLCxkx44dREdH895773HnnXeyevVqFi1axIMPPkhUVBR791rWs//iiy8YPnw4ffv2ZcWKFYClY/ymm24iPDycIUOGsHTpUoDysspccsklLFu2jFmzZpGfn09UVBTXXXddtXGfOHECf3//8seXX345Q4cOZdCgQbzxxhsAlZb3wQcfEBERQWRkJDfccEP5+cuXL2fUqFH07NmzylqCm5sbM2bM4IUXXjhtX2JiIueeey4RERGMGzeOgwcPApZax8yZMxkxYgQPPfQQ06dP5/bbb2fkyJH07NmTZcuWcfPNNzNgwACmT59eXt7tt99OTEwMgwYN4p//rHzke2hoKGlpabz22mtERUURFRVFWFgYY8eOrbKMl156iUOHDjF27Njy48rKAXj++ecZPHgwgwcPLq8xJiYmMmDAAG699VYGDRrE+PHjyc/Pr/b1qY1mNzdRs5N3HBJXwKi7wMMHfnsKDm+CLpE1n6uc5olvt7H90IkGLXNg13b889JBle7r2rUrbm5uHDx4kNWrV3PGGWeQkpLCmjVr8PPzIzw8HA8Pj/Ljy2oJl1xyCVdddVX59uLiYtatW8f333/PE088wS+//MK8efMQEbZs2cLOnTsZP348u3fvrjLO2bNn8/LLLxMfH1/lMWPHjsUYw759+/j88/IVNHnnnXcICAggPz+fYcOGceWVV55W3rZt23jqqadYvXo1gYGBHD9+vPz8w4cPs3LlSnbu3MnEiRNPuTZbf/3rX4mIiOChhx46Zftdd93FtGnTmDZtGu+88w533303CxcuBCxDh1evXo2rqyvTp08nIyODNWvWsGjRIiZOnMiqVat46623GDZsGPHx8URFRfH0008TEBBASUkJ48aNY/PmzURERFQa08yZM5k5cyZFRUWce+65/O1vfwOotIy7776b559/nqVLlxIYGHhKOXFxcbz77rusXbsWYwwjRozg7LPPxt/fnz179vDJJ5/w5ptvMmXKFL788kuuv/76Kl+n2tCagaPt/hFKi2HApTB8Bnj6ae1AVWrUqFGsXr26PBmcccYZ5Y9Hjx5tVxmTJk0CYOjQoSQmJgKwcuXK8jeM/v37ExISUm0ysMfSpUvZunUrW7Zs4c477yQnJwewfOKNjIxk5MiRJCUlsWfPntPO/e2335g8eXL5m2BAQED5vssvvxwXFxcGDhzI0aNHq3z+du3aceONN/LSSy+dsn3NmjVce+21ANxwww2sXPnn8tCTJ0/G1dW1/PGll16KiBAeHk6nTp0IDw/HxcWFQYMGlf/uPv/8c6KjoxkyZAjbtm2zq1nunnvu4dxzz+XSSy+tUxkrV67kiiuuwMfHB19fXyZNmlReywsLCyMqKgo49TVuCFozcLSdi6FdN+gaDSIwYgYsfw6O7YCOA5wdnapCVZ/gHams32DLli0MHjyY7t2789///pd27dpx00032VWGp6cnAK6urjW2jbu5uVFa+ufiZXUZm96rVy86derE9u3bycvL45dffmHNmjV4e3tzzjnn1LrMsvjBcgNVde69916io6Pt/t34+PhU+lwuLi6nPK+LiwvFxcXs37+fOXPmsH79evz9/Zk+fXqN1/Pee+9x4MABXn75ZYA6lVEd2zhdXV0btJlIawaOdDIXEn6B/pdYEgHAiNvB3QdWPO/c2FSTM2rUKBYvXkxAQACurq4EBASQmZnJmjVrGDVq1GnHt23bluzs7BrLHTNmDPPnzwdg9+7dHDx4kH79+hEaGkp8fDylpaUkJSWxbt2fgxvc3d0pKiqqsexjx46xf/9+QkJCyMrKwt/fH29vb3bu3Mkff/xRaXnnnnsuX3zxBenp6QCnNBPVRkBAAFOmTOHtt98u3zZq1Cg+/fRTAObPn8+YMWPqVDZY+kN8fHzw8/Pj6NGjLFmypNrj4+LimDNnDh999BEuLi41llHV6zdmzBgWLlxIXl4eubm5fP311/W6DntpMnCkhF+guMDSRFTGpwMMuxm2LoD0vc6LTTU54eHhpKWlMXLkyFO2+fn5ndauDDB16lSee+45hgwZUt6BXJk77riD0tJSwsPDufrqq3nvvffw9PRk9OjRhIWFMXDgQO6++26io6PLz5kxYwYRERFVdiCPHTuWqKgoxo4dy+zZs+nUqRMTJkyguLiYAQMGMGvWrFOuw7a8QYMG8eijj3L22WcTGRlZ3rZeF/fff/8po4rmzp3Lu+++S0REBB9++CH/+9//6lx2ZGQkQ4YMoX///lx77bU1NtW9/PLLHD9+vPx3c8stt1RbxowZM5gwYUJ5B3KZ6Ohopk+fzvDhwxkxYgS33HILQ4Y4fg7PZrcGckxMjGk2i9t8easlITywB1xtWuSyj8KL4RAxBS572XnxqVPs2LGDAQO06U41f5X9LYtInDEmpqpztGbgKMUnYfcP0P+iUxMBQNtOMHQabPoEMpMqP18ppRqRJgNH2b8cCk9A/0sr3z/qbkBgVd2rsUop1VA0GTjKzm/Bwxd6nlP5/vbdIeoa2PCBZboKpZRyIk0GjlBaAju/gz7jwd2r6uPOvA9Ki2D13MaLTSmlKqHJwBGS1kFuKgy4pPrjAnpC+GSIfQdy0xsnNqWUqoQmA0fY8S24elhqBjU5829QlA9/vOL4uJRSqgqaDBqaMZZk0HMseLat+fiO/WHgRFj3BuRnOjw81TQ11hTWiYmJtGnThqioKCIjIxk1ahS7du2q8ZyPP/64xmuwnWitKsuWLUNE+Pbbb8u3lU2Q15h8fX0BOHToUJXzH1UnMzOTV1758wNcXctpSjQZNLQjmyHr4Kk3mtVkzAOWkUfr3nBcXKpJa8wprHv16kV8fDybNm1i2rRpNU4vbW8ysFdwcDBPP/10nc8vKal0ddw66dq1q11rKFRUMRnUtZymRJNBQ9vxLYgL9LvI/nO6REDfCZamosIaphcwBlI2wKK74N894LPrtUbRAjhyCuvq2E5BnZiYyJgxY4iOjiY6Oro8Oc2aNYsVK1YQFRXFCy+8QElJCQ888ACDBw8mIiKCuXP/HAAxd+5coqOjCQ8PZ+fOnZU+Z2RkJH5+fvz888+n7fv1118ZMmQI4eHh3HzzzRQWFgKWWsfDDz9MdHQ0X3zxBaGhofz9738nKiqKmJgYNmzYwAUXXECvXr147bXXAMjJyWHcuHHl8VS2CE9iYiKDBw8G4JZbbimfgjooKIgnnniiyjJmzZrF3r17iYqK4sEHHzylnOqmDJ80aRITJkygT58+p8246mw6UV1D2/EthIy2TDtRG2MegLfPs3Qmj65kRbTCbNjyBcS+a6l9uHtDr3Nh1xJ44xyY8oElqaiGsWQWHNnSsGV2DocLZ1e6y5FTWFdU9iaWnZ1NXl4ea9euBaBjx478/PPPeHl5sWfPHq655hpiY2OZPXs2c+bMYfHixQC8+uqrJCYmEh8fj5ub2ylzCwUGBrJhwwZeeeUV5syZw1tvvVXp9T766KP84x//4Pzzzy/fVlBQwPTp0/n111/p27cvN954I6+++ir33nsvAB06dGDDhg2A5c24R48exMfHc9999zF9+nRWrVpFQUEBgwcPZubMmXh5efH111/Trl278mk+Jk6cWOUawWWxHjhwgAkTJjB9+vQqy5g9ezZbt24tn5bbdvbQ6qYMj4+PZ+PGjXh6etKvXz/uuusuunfvXjEUp9CaQUNK2wOpO2vXRFSm+zDLPQmr51o6lMsc2giL7oY5/WDxfWBK4aI5cP9OmDofpn9vmf/o7fNhw4cNdimq8TlqCuuKypqJ9u7dy4svvsiMGTMAKCoq4tZbbyU8PJzJkydX2QT1yy+/cNttt+HmZvksaTsFtT3PD3DWWWcBnDLF9K5duwgLC6Nv374ATJs2jeXLl5fvv/rqq08pY+JEy+q54eHhjBgxgrZt2xIUFISnpyeZmZkYY3jkkUeIiIjgvPPOIyUlpdppscGSkCZPnszcuXMJCQmpUxnVTRk+btw4/Pz88PLyYuDAgRw4cKDashqT1gwa0g5rp1j/i+t2/lkPwnsXW5qL2gRA3LuWhXDc2sDgKyHmJug29M8ZUAF6jIDbVsCXf4FFd0LSH5Zk4d6m/tfTmlXxCd6RGnsKa7C8oZaV/cILL9CpUyc2bdpEaWkpXl7V3CPTAM//6KOP8tRTT5UnlZrUdgrq+fPnk5qaSlxcHO7u7oSGhtY4ffTMmTOZNGkS5513HkCdyqhOxSmoG3oJzvrQmkFD2rnYsm6BX3Ddzg8ZDT3OgF+fhMX3Qknxn7WAy+dBcMypiaCMbxDc8LWlqWnjR5ZawvF99boU1fgcNYV1dVauXEmvXr0AyMrKokuXLri4uPDhhx+Wd9RWfJ7zzz+f119/vfyNrK5TUI8fP56MjAw2b94MQL9+/UhMTCQhIQGADz/8kLPPPrvO15aVlUXHjh1xd3dn6dKlNX4KnzdvHtnZ2eUd9NWVUd3vvqopw5s6TQYNJSsZUuJqvtGsOiJw0XMwYib85We4fRUMvxXatK/5XBdXGPcPuPZzy+R3r59juQtaNRuOmsK6orI+g8jISB555JHytvI77riD999/n8jISHbu3Fn+STwiIgJXV1ciIyN54YUXuOWWW+jRo0f5Gsb1GWn06KOPkpRkmazRy8uLd999l8mTJ5evOjZz5sw6l33dddcRGxtLeHg4H3zwAf3796/2+Dlz5rBly5byTuTXXnutyjI6dOjA6NGjGTx4MA8++OAp5VQ1ZXhTp1NYN5S1r8OSh+DOWAjs49xYMg7A5zfC4XgYfS+c+4/TZ05Vp9EprFVLoVNYO9OObyGov/MTAYB/CNz8Iwy9CVa9CB9cZllDQSmlqqDJoCHkpsOBVZblLZsKdy+49EW4/DVL89XrY+DEIWdHpZRqorTtoCHs+t4y5LMuQ0odLeoay/0Hmz+Htl2cHU2TZ4ypchy6Us1BXZv+tWbQEHYuBr8e0CXS2ZFUrtMgOP+JykciqXJeXl6kp6fX+Z9JKWczxpCenl6nYcFaM6ivwmzY+xsMu0XfbJu54OBgkpOTSU1NdXYoStWZl5cXwcG1H96uyaCikmKIfRsQ6HM+BIRVf/yen6DkZNNsIlK14u7uTlhYDa+3Ui2UJgNbhTmw4CbLGzzAEqBDb8u6BL3Ps9wUVnHlsh2LwTsQuo9o9HCVUqqhaDIoc+IwfDwFjm6DS16AsLNhz8+Q8DOsf9syRYS7N4SdZUkMfc4H386WxDH4SstNX0op1UxpMgBLApg/BQoy4drPLG/0AB16wciZcDIPEldaEsOen2D3D5b97brByRxtIlJKNXuaDPYutdyt6+EDNy2pfBpoD2/oO97yZZ6F9L3WxPCzZR6isLMaP26llGpArTsZbPjQMiFcYD+47nP7JpgTgcDelq+Rtzs8RKWUagytMxkYA0ufhuXPWdYqnvIBeLVzdlRKKeU0Dr3pTEQmiMguEUkQkVlVHDNFRLaLyDYRabiFVqtSXAhfzbAkgiE3wHVfaCJQSrV6DqsZiIgrMA84H0gG1ovIImPMdptj+gB/B0YbYzJEpKOj4gEgPwM+vR4OrLTM5Dnmfr1RTCmlcGwz0XAgwRizD0BEPgUuA2zX0rsVmGeMyQAwxhxzWDQZiTB/suX7pLcgYrLDnkoppZobRzYTdQOSbB4nW7fZ6gv0FZFVIvKHiEyorCARmSEisSISW+epArZ9DTnH4IaFmgiUUqoCZ3cguwF9gHOAYGC5iIQbYzJtDzLGvAG8AZbFber0TKPvhfAp4FcxHymllHJkzSAF6G7zONi6zVYysMgYU2SM2Q/sxpIcGp6IJgKllKqCI5PBeqCPiISJiAcwFVhU4ZiFWGoFiEgglmYjXcldKaUamcOSgTGmGLgT+BHYAXxujNkmIk+KyETrYT8C6SKyHVgKPGiMSXdUTEoppSonzW0hj5iYGBMbG+vsMJRSqlkRkThjTExV+3WlM6WUUpoMlFJKaTJQSimFJgOllFJoMlBKKYUmA6WUUmgyUEophSYDpZRSaDJQSimFJgOllFJoMlBKKYUmA6WUUmgyUEophSYDpZRSaDJQSimFJgOllFJoMlBKKYUmA6WUUmgyUEophSYDpZRSaDJQSimFJgOllFJoMlBKKYUmA6WUUmgyUEophSYDpZRSaDJQSimFJgOllFJoMlBKKYUmA6WUUmgyUEophSYDpZRSaDJQSimFJgOllFLYkQxE5FIRqVPSEJEJIrJLRBJEZFYl+6eLSKqIxFu/bqnL8yillKofe97krwb2iMizItLf3oJFxBWYB1wIDASuEZGBlRz6mTEmyvr1lr3lK6WUajg1JgNjzPXAEGAv8J6IrBGRGSLStoZThwMJxph9xpiTwKfAZfWOWCmlVIOzq/nHGHMCWIDlDb0LcAWwQUTuqua0bkCSzeNk67aKrhSRzSKyQES6V1aQNfnEikhsamqqPSErpZSqBXv6DCaKyNfAMsAdGG6MuRCIBO6v5/N/C4QaYyKAn4H3KzvIGPOGMSbGGBMTFBRUz6dUSilVkZsdx1wJvGCMWW670RiTJyJ/qea8FMD2k36wdZttGek2D98CnrUjHqWUUg3Mnmaix4F1ZQ9EpI2IhAIYY36t5rz1QB8RCRMRD2AqsMj2ABHpYvNwIrDDvrCVUko1JHuSwRdAqc3jEuu2ahljioE7gR+xvMl/bozZJiJPishE62F3i8g2EdkE3A1Mr03wSimlGoY9zURu1tFAABhjTlo/6dfIGPM98H2FbY/Z/Px34O92xqqUUspB7KkZpNp8kkdELgPSHBeSUkqpxmZPzWAmMF9EXgYEy3DRGx0alVJKqUZVYzIwxuwFRoqIr/VxjsOjUkop1ajsqRkgIhcDgwAvEQHAGPOkA+NSSinViOy56ew1LPMT3YWlmWgyEOLguJRSSjUiezqQRxljbgQyjDFPAGcAfR0bllJKqcZkTzIosH7PE5GuQBGW+YmUUkq1EPb0GXwrIu2B54ANgAHedGRQSjUkYwxlfV1KqcpVmwysi9r8aozJBL4UkcWAlzEmqzGCU6o+kjPyePTrrcQmHueCQZ25amgwI3t2wMVFE4NSFVWbDIwxpSIyD8t6BhhjCoHCxghMqboqLTXMX3uA2Ut2YoBxAzrx8/ajfLUxhW7t23Dl0GCuig6mRwdvZ4eqVJNhTzPRryJyJfCVMcY4OiCl6iMxLZeHvtzMuv3HGdMnkGeuCKd7gDcFRSX8uO0IC+KSmfvbHl76dQ8jwgK4amgwF4V3wcfTrlHWSrVYUtP7u4hkAz5AMZbOZAGMMaad48M7XUxMjImNjXXGU6smrKTU8M7K/fz35124u7rwj4sHMjkmuNK+gkOZ+Xy1IZkFcckkpufh7eHKReFduDI6mM5+XuQWFlu+ThaTW1hi/bnklO09AryZcVYvJ1ypUnUjInHGmJgq9ze3D/uaDFRFu49m89CCzcQnZXLegI48fUU4ndp51XieMYbYAxksiE1m8eZD5J4sqfEcDzcXXEXILyoh7v/Oo4OvZ0NcglIOV1MyqLFuLCJnVba94mI3SjW2opJSXv99Ly/9moCPpyv/mxrFxMiudo8cEhGGhQYwLDSAf04cyLJdqRQUleDj6YavpxveHq6W755u+Hq44e3pirurC7GJx7nqtTXEHchg/KDODr5KpRqHPQ2lD9r87IVlofs44FyHRKRUDQqKStiaksVj32xj++ETXBzRhScmDiKwHp/SvT3cuCjcvttnBnfzw8PVRZOBalHsmajuUtvH1kXrX3RUQKrlSc0upKCoBA83FzxcXfB0t3x3c638nseTxaUcyswnKSOP5Ix8ko5bv1sfp2ZbBrQFtfXkteuHMmFw474he7m7MrhbO+IOZDTq8yrlSHUZQpEMDGjoQFTLcaKgiD/2prMqIY0VCWnsS82t9DgXAU83VzzcXPB0c8HDzYXiEsPR7AJsu7JcXYSu7b3o7u/N2H5BdPf3pnuAN2P7dcTP272RrupUQ0P8eX/NAQqLS/B0c3VKDEo1JHv6DOZiuesYLNNXRGG5E1kpAAqLS9h4MJNVCWmsTEhjU1ImpQbauLsyomcAU4d1x9/bg5MlpZwsLqWw2PLd8nOJ5eeSUgqLSnFxEbq1b0Owfxu6B3gT7N+Gzu28qqxFOMvQkADeXLGfrSlZDA0JcHY4StWbPTUD26E7xcAnxphVDopHOUlOYTF/7E3nj33p5BWV4OHqgrur4OHmgrur5cvD1cXmsZCZV8TKhDTW7T9OflEJri5CZLAfd47tzejegQzp4Y+HW9N6E28oQ0P8AYg7kKHJQLUI9iSDBUCBMaYEQERcRcTbGJPn2NCUIxWXlLI5JYsVu9NYmZDKxoOZFJcaPN1caOvlTpH1U3xRSSnFpVUPP+4V5MOUmGBG9w5kZK8OtPNyTrNNYwtq60lIB29iEzOYUel4O6WaF7vuQAbOA8pWOGsD/ASMclRQyjEOpOeyfE8aK/eksnpvOtkFxYjA4K5+3HpWT8b0CWRoiP9pbeClpYaTJZbEUFRiypOEp7sLHdvWPJ6/pRoa4s/y3ak6EZ5qEexJBl62S10aY3JERCd1acKy8ovYl5rDvtRc9qXlsD8tly0pWSQdzwegW/s2XBzehTP7BDKqVyABPh7VlufiIni5uOLlrh2ltmJCAvhqQwoH0vMIDfRxdjhK1Ys9ySBXRKKNMRsARGQokO/YsJQ9DmflsyU5i/1pueVv/PtSc0nPPVl+jKuL0CPAm4Fd2nHrmJ6c2TuQsEAf/STbAMr6DWIPZGgyUM2ePcngXuALETmEZV6izliWwVROUlJqeH35Xl74eTdFJZb2/EBfD3oG+nLegE70DPKhZ5AvPYN86O7v3WI7cZ2tT0df2nm5EXcgg6uGBjs7HKXqxZ6bztaLSH+gn3XTLmNMkWPDUlVJOp7HfZ/FE3sgg4vCO3PrmJ70DPLFr03r6LhtSlxchOgQf+IOHHd2KErVmz33GfwVmG+M2Wp97C8i1xhjXnF4dKqcMYYv4pJ5YtE2XER4fkokVwzpps09Tja0hz/LdqWSlVfktBvglGoI9rQf3Gpd6QwAY0wGcKvDIlKnSc8p5LYP43howWYGd/Njyb1jmBRd+fTMqnENDbX0G2xI0qkpVPNmT5+Bq4hI2cI2IuIKVD/8RDWY33Ye5aEFWziRX8SjFw3gL2eG6bKNTUhU9/a4ughxiRmM7dfR2eGoZiQj9ySv/r6XrLwinpkUjquT/6/tSQY/AJ+JyOvWx7cBSxwXkgLIO1nMU9/t4OO1B+nfuS0f3TKc/p2dsp6Qqoa3hxsDu7QjVvsNlJ0Kikp4Z9V+Xl22l+yCYgB6dPDmr2N7OzUue5LBw8AMYKb18WYsI4qUg2w8mMF9n8Vz4Hget53Vk7+N76uToTVhQ0P8+Wx9EkUlpbg3sTmUVNNRXFLKlxuSeeHnPRw5UcC4/h15aEJ/XvptDy/8vJszewcS2b290+KzZzRRqYisBXoBU4BA4EtHB9aaFJWUsiUli7X7jrN2fzor9qTRuZ0Xn9w6kpE9Ozg7PFWDmFB/3ludyI7DJ4gIbu/scFQTY4zhlx3HePaHnew5lkNU9/b8b2oUI6z/289cHs7GAxnc+1k8i+8602nrcVf5rCLSF7jG+pUGfAZgjBnbOKG1XIXFJWxKymLtvnTW7j9O3IEM8ossSy727ujLTaNCufu8Pq1mnp/mrvzms8QMTQbqFHEHjjN7yU7WJ2bQM9CH166P5oJBnU8Z/OHn7c7zV0dxzZt/8OS32/nPVRFOibW6FLQTWAFcYoxJABCR+xolqhZoa0oWP28/ytr96Ww8mElhcSkA/Tu35eph3RkRFsCwsIB6rdalnKOLXxu6tW9D3MEMbibM2eGoJiDhWA7P/rCTn7YfJaitJ09fMZgpMd2rbEYc2bMDt5/di1eW7eWcfkFcaOeqew2pumQwCZgKLBWRH4BPsdyBrGppc3Imk15ZTakxDOzajutHhjAiLIDhYQG099aBWS1BdIg/6/cf10nrFBsOZjD19T/wcHPh/vP78pcxYXh71Nz0c+95fVmZkMasr7YQ1aM9XfzaNEK0f6qyt8sYs9AYMxXoDyzFMi1FRxF5VUTG21O4iEwQkV0ikiAis6o57koRMSISU8v4m7y8k8Xc82k8QW09WfvIeSy+awz/uGQg4wd11kTQgsSE+HPkRAEpmTptV2uWml3I7R/F0cnPk98eOJu7xvWxKxEAeLi58OLVUZwsLuX+zzdRWs3U8Y5Q49AHY0yuMeZj61rIwcBGLCOMqmW9H2EecCEwELhGRAZWclxb4B5gbS1jbxb+tXg7iem5/HdKJEFttQmopbJd7Ea1TsUlpdz1yQYy84p47fqhdZrevWeQL49PHMjqvem8uWKfA6KsWq3GwRljMowxbxhjxtlx+HAgwRizzxhzEksz02WVHPcv4D9AQW1iaQ5+2HqET9YlcdtZvRjVK9DZ4SgH6t+5Ld4erpoMWrFnf9zFH/uO88wV4Qzq6lfncqbEdGfCoM7M+WkXW1OyGjDC6jlyUHQ3IMnmcbJ1WzkRiQa6G2O+q64gEZkhIrEiEpuamtrwkTrAkawCZn21mfBufvzt/L7ODkc5mJurC0N6tCc2UZNBc3KioIh/LNzKmr3p9Srn+y2HeWP5Pq4f2YMr6zmDrYjw70nhdPDx5O5PN5J/sqRe5dnLaXfIiIgL8Dxwf03HWmsjMcaYmKCgIMcHV0+lpYb7v4insKiUF6dG6RTSrcTQkAB2HjlBTmGxs0NRdjhRUMQNb6/jwz8OcMPba/k8NqnmkyqRcCybB7/YxJAe7XnskkENEpu/jwfPT4lkf1ou//pue4OUWRNHvkulAN1tHgdbt5VpCwwGlolIIjASWNQSOpHfXrmfVQnpPHbpQHoF+To7HNVIhob4U2og/mCms0NxutJSw/LdqU02MZYlgu2Hsnjh6kjO6NWBhxZs5tkfdtaq4zansJjbPozDy92VV66LbtAPfqN6BzJjTE8+XnuQn7YdabByq+LIZLAe6CMiYSLigWWY6qKyncaYLGNMoDEm1BgTCvwBTDTGxDowJofbdiiLZ3/cyQWDOjF1WPeaT1AtxpAe7RGh1c9TtDUliytfW82N76zjb5/FY53jssmwTQTzro3miiHBvDN9GNcM784ry/Zy16cbKSiquWnGGMODX2xif1ouc68d4pChoH8b35dBXdvx8JebOXbCsd2qDksGxphi4E7gR2AH8LkxZpuIPCkiEx31vM6Uf7KEuz/ZSICPB7MnReh481amnZc7/Tq1bbWdyFn5Rfzzm61MfHklScfzuDSyKz9tP8qiTYecHVq5iolg/CDLNGvuri48c0U4f7+wP99tPsw1b/5BWk5htWW9sXwfS7YeYdaF/R02QMTTzZX/TR1CflEJ93/h2OGmDm3MNsZ8b4zpa4zpZYx52rrtMWPMokqOPaep1AqyC4ooKimt9XlPf7+dvam5/HdyFP41LDKvWqahIf5sPJhJSSOPEXcmYwxfb0xm3H9/t7S/jwzh1/vP4cWro4ju0Z7Hvtnm8E+19sjKrzwRlBERbju7F69eF832Qye44pVVJBzLrrSs1Qlp/OeHneWrDTpS746+/OOSgazYk8aHfxxw2PNoz2YFWXlFnPXsUs78z2/MW5pAhs3i8tX5ZftRPvrjILeOCePMPjqMtLWKCfUnp7CYXUcqfxNpaXYfzWbqG39w32ebCPZvw6I7z+SJywbj18YdVxfhucmRFBSV8MjXW5zaXJSVX8SN71SdCGxdGN6Fz247g/yTpVzxympWJaSdsv9wVj53fbKRsEAfnr0qslFaAK4d3oMHxvflwsGOmzBak0EFb63cR0ZeEaEdfHjux12M/Pev/P2rLew5WvU/97HsAh76cjMDu7TjgQv6VXmcavliQgIAiDtYu6aiT9YdZOaHcY4IySFyC4t55vsdXPS/Few6ms2/J4Xz1e2jGNzt1PH1vYJ8efCCfvyy4xhfb0ypojT7JBzL4dz/LmP6u+v4Jj6FvJP2dU7XJhGUierenoV/HUUXPy+mvbOOz9YfBCyTTN7+0QYKikp4/Yah+DbSDKMiwp3n9qFju9rfyGYv58yV2kRl5p3k3VWJXBTemVeuG8ruo9m8u2o/X21I5pN1BxnTJ5Cbzwzj7D5B5auNlZYaHvhiM7mFxbx0TZSuO9DKBfu3IaitJ3GJx7lhZIhd56xKSOP/Fm6lpNRwPPckAU24idEYw/dbjvCvxds5cqKAqcO689CE/tXGfNPoMH7cdoTHF21jVK9AOvvV/g0tOSOPG95eS2FxKQUns7nn03i8PVwZP7ATlw3pxpjegbhVMglcXRJBmWB/bxbcPoq/zt/Aw19uYX9aHtkFRcQnZfLKddH07ti21tfRlGkysPHmin3knizmnnGWm8T6dmrLvydF8OAF/flk3UHeX53ITe+up1eQDzeNDmNSdDc+XZfE8t2p/OvywS3uj0PVnogQE+JPrJ2dyEnH87jz4w14ubmQe7KEhGM5DA8LcHCUtZeWU8g38YdYEJfMjsMnGNilHfOuiy6fhqM6ri7Cc1dFMuF/y/n7V5t5Z/qwWjWtpGYXcsPb68gpLObTGSMZ0Lkd6xOPszD+EN9tPsTC+EN08PHgkoguXD6kG1Hd2yMi9UoEZdp5ufPO9GH8c9E2Xvt9LwC3ndWTi5wwq6ijSVMb9lWTmJgYExvb8P3Mx3NPMuY/v3FO/47Muza60mNOFpfy/ZbDvL1yP1tSsvBr407+yRLO6hvImzfG6OghBcBbK/bx1Hc7WPvIODpVU63PP1nCpFdXk5yRxyvXRXPD2+t45opwrh3RoxGjrdrJ4lJ+23mUBXEpLNt1jOJSQ0SwH9cM78HkocGVfhKvzrur9vPEt9t57qoIJsfYN+w6K7+IqW/8wf60HD76ywhiQk9NlIXFJSzblco38Sn8suMYJ4tLCengzWVR3fh9d2q9EoEtYwwfrDnArqPZPDlxUK2vvSkQkThjTJX3cWnNwOrNFfvIKyrh3nF9qjzGw82Fy4d047KorsQdyODtlftJTM/jP1fqMFL1J9tJ66r6BGmM4aEvN7PzyAnemT6M0b0CaePuSsKxnMYMtdK4tqRk8WVcMt9sOkRmXhEd23rylzPDuHJoMH071b32O+2MUJZsPcKT327nzD6BNY7LzztZzM3vrSfhWDZvTRt2WiIAy9DLCwZ15oJBnTlRUMQPW4+wcGMKc3/bg5uLNEgiAEuNb9qo0HqX05RpMsBSK3h/dSKXRHSljx1/7CJCTGhApX+cSg3q6oenmwuxiVUngzdX7OPbTYd48IJ+jO3XEYBeHX1ISHVOMjh2ooCvN6bw5YZkdh/NwcPNhfEDO3Hl0OAq2+Nry8VFmGNtLnr4yy28f1PVzUWFxSXc9mEcGw9m8PK10Zzdt+ZpaNp5uTMlpjtTYrpzJKuAnMJienfUGQDspckAy80j+UUl3DOut7NDUS2Ah5sLkcHtqxxRtGJPKrOXWMao33FOr/LtvYN8Wbe/8e9eXrLlMPd9Hk9BUSnRPdrz9BWDuSSiK35tGn7Z1R4dvJl1YX8e+2Ybn61PYurw05vEiktKuffTeFbsSePZKyPq1D5fl07q1q75NXw1sPScQj5Yk8jEyK7aAawazNBQf7alZJ024+TB9Dzu/HgjfTq25bkKY9T7dGrLoawCchtpPh9jDK//vpc7Pt7AwC7t+PX+s/nqjtFcNyLEIYmgzPUjQjijZwee+m7HaYsBGWN45OstLNl6hP+7eABTdEqXRtPqk8Eby/dRUFTCXedW3VegVG3FhPhTXGrYnJxZvi23sJgZH1oGP7xx41B8KoxRL5vUcG8jNBUVl5Ty6MKt/HvJTi4a3IWPbx3ZaJMqurgIz14VgTGGhxdsLr8ZzRjDU9/t4PPYZO4+tze3OPjOXnWqVp0M0nIK+WDNAWutQNsWVcOJ7mHpRC4bYmqM4cEFm9h9NJu51wwhpIPPaeeU/Q3uOerYZJBdUMTN78fy8dqD3HFOL+ZeMwQv98a9P6Z7gDePXDyAlQlpfLzOckPX3N8SeHvlfqaPCuU+XQOk0bXqPoPXf99LYXEJd1czgkipuvD38aBXkE/5pHWv/r6X77cc4e8X9uesKjpDQzp44+4qDu1ETsnM5y/vrWfPsRxmTwqvtM2+sVw7vAdLthzhme92cCgzn3lL9zJpSDceu2Sgjs5zglZbMziWXcCHfxzg8qhu9NQ1B5QDxIQEsOFgBkt3HuO5H3dxaWRXZpxVddOHu6sLoR18HDa8dEtyFpfPW0VKRj7v3TTMqYkALKPyZl8Zjogwb+lezh/YiWeviii/u181rlabDF7/fR9FJYa7tFagHGRoiD+ZeUXM/CiO/p3b8awd96P07ujrkGTw8/ajTHl9DR6uLnx5xyjG9GkaKwYG+3vz/JRIrh3Rg7nXDGmWN3O1FK2ymejYiQI+stYKwgJPb7tVqiEMDbX0G3h7uPLGDUNp41Fzu3yfjr78uO0IhcUlDTLPlTGGd1cl8q/vthPRzY83p8XQsW3TGnY5flDnBrkxTNVPq0wGr/2+j+JSw13n6n0FynF6Bvpwy5lhTBjcme4B3nad06ujL6UGEtPy6Ne5fkOdi0tK+dfi7by/5gAXDOrEi1cPsSshqdap1SWDYycKmL/2AFcM6Uao1gqUA4kI/3fJwFqdUz6i6Fh2vZPB68v38f6aA9w6JoxZFw7AVdviVTVaXTJ4ZdlerRWoJqtXkC8iNEi/weq9aQzq2o5HL65dQlKtU6vqrTmSVcDH6w5yZXS3Ssd5K+VsXu6udPf3rncyKC01bE7KIrJ7+4YJTLV4rSoZvLosgdJSo3cbqyatIUYU7U/PJbuwmKjg9g0TlGrxWk0yOJyVzyfrkrhqaLDdnXlKOUPvjr7sS8ulpLTua41sSsoEIKK7X/UHKmXVapLB/D8OUmoMfx2rfQWqaevd0ZeTxaUkHc+rcxmbk7Pw9nClj06+qOzUajqQ7zmvD2f3C9JagWry/hxRlFPnEW/xSZkM7uqnI4iU3VpNzcDd1YVhuhiNagbKkkFd+w1OFpey/fAJIrWJSNVCq0kGSjUX7bzc6dTOs87JYNeRbE4WlxKhnceqFjQZKNUEWUYUZdfp3E3WNRSidFipqgVNBko1Qb2DfNmbmlu+8EttbE7OxN/bnWD/6hecV8qWJgOlmqDendqSU1jMkRMFtT53k/VmM10TQNWGJgOlmqDeQXVb9Sy3sJg9x7K1v0DVmiYDpZqguo4o2pqSRamBKB1JpGpJk4FSTVCgrwftvd1rvQTm5uQsAK0ZqFrTZKBUEyQi9A7yJaGWzUTxyZl0a9+GQF9PB0WmWipNBko1Ub07+tahZpCpN5upOtFkoFQT1bujL8dzT5KeU2jX8ek5hSQdz9cmIlUnDk0GIjJBRHaJSIKIzKpk/0wR2SIi8SKyUkR0FQ6lrGrbibw5xdJfEKnJQNWBw5KBiLgC84ALgYHANZW82X9sjAk3xkQBzwLPOyoepZqb8mRgZ1PRpqRMRCA8WJuJVO05smYwHEgwxuwzxpwEPgUusz3AGHPC5qEPUPcJ3JVqYbr6tcHbw9X+mkFyFr2DfPH1bDWTEasG5Mhk0A1IsnmcbN12ChH5q4jsxVIzuLuygkRkhojEikhsamqqQ4JVqqlxcRF6Bdm36pkxhk1JmdpfoOrM6R3Ixph5xphewMPA/1VxzBvGmBhjTExQUFDjBqiUE9m7BGZKZj7puSf1ZjNVZ45MBilAd5vHwdZtVfkUuNyB8SjV7PTu6MvhrAJyCourPU5vNlP15chksB7oIyJhIuIBTAUW2R4gIrYr018M7HFgPEo1O2WdyHtrqB1sSsrEw9WF/l10mUtVNw5LBsaYYuBO4EdgB/C5MWabiDwpIhOth90pIttEJB74GzDNUfEo1RzZLoFZnU3JmQzo0hZPN9fGCEu1QA4ddmCM+R74vsK2x2x+vseRz69UcxcS4I27q1Tbb1BSatiSnMWVQ4MbMTLV0ji9A1kpVTU3VxfCAn2qTQb7UnPIPVmi/QWqXjQZKNXE1bQEZnxSJqDTVqv60WSgVBPXO8iXg8fzKCgqqXT/5uQsfD3d6Bno28iRqZZEk4FSTVzvTm0pNZCYnlvp/k3JmQzu1g4XF13mUtWdJgOlmrjqlsAsLC5hx+ETRHZv38hRqZZGk4FSTVzPIB9EKp+9dMfhbIpKjM5UqupNk4FSTZyXuys9Arwrnb10c3ImgNYMVL1pMlCqGahqCcxNSVkE+nrQ1c/LCVGplkSTgVLNQO+OvuxPy6W4pPSU7ZuSM4kMbo+Idh6r+tFkoFQz0LujLydLSknKyC/fll1QxN7UHL3ZTDUITQZKNQPlcxQd/fPmsy0pWRgDkXqzmWoAmgyUagZ6VbIEpk5brRqSJgOlmoF2Xu50bud1yvDSTUmZ9AjwJsDHw4mRqZZCk4FSzUTFVc82J2cREaxNRKphaDJQqpno3dGXvcdyMMaQml1ISmY+UXp/gWogDl3PQCnVcHp39CX3ZAmHswrYcfgEoP0FquFozUCpZsJ21bNNSZm4CAzu1s7JUamWQpOBUs1EWTJIOJbDpuQs+nZqi7eHVu5Vw9BkoFQz0cHHg/be7iQcy2ZzcqZ2HqsGpclAqWZCROjT0Zdlu1LJyCvSyelUg9JkoFQz0rujL4ezCgB02mrVoDQZKNWM9LIudOPh5kK/zm2dHI1qSTQZKNWM9OlkSQCDurbD3VX/fVXD0b8mpZqRshFF2kSkGpqOS1OqGenq58V95/Xl4ojOzg5FtTCaDJRqRkSEe87r4+wwVAukzURKKaU0GSillNJkoJRSCk0GSiml0GSglFIKTQZKKaXQZKCUUgpNBkoppQAxxjg7hloRkVTgQB1PDwTSGjCcpqClXVNLux5oedfU0q4HWt41VXY9IcaYoKpOaHbJoD5EJNYYE+PsOBpSS7umlnY90PKuqaVdD7S8a6rL9WgzkVJKKU0GSimlWl8yeMPZAThAS7umlnY90PKuqaVdD7S8a6r19bSqPgOllFKVa201A6WUUpXQZKCUUqr1JAMRmSAiu0QkQURmOTue+hKRRBHZIiLxIhLr7HjqQkTeEZFjIrLVZluAiPwsInus3/2dGWNtVHE9j4tIivV1iheRi5wZY22JSHcRWSoi20Vkm4jcY93eLF+naq6n2b5OIuIlIutEZJP1mp6wbg8TkbXW97zPRMSj2nJaQ5+BiLgCu4HzgWRgPXCNMWa7UwOrBxFJBGKMMc32RhkROQvIAT4wxgy2bnsWOG6MmW1N2v7GmIedGae9qriex4EcY8wcZ8ZWVyLSBehijNkgIm2BOOByYDrN8HWq5nqm0ExfJxERwMcYkyMi7sBK4B7gb8BXxphPReQ1YJMx5tWqymktNYPhQIIxZp8x5iTwKXCZk2Nq9Ywxy4HjFTZfBrxv/fl9LP+ozUIV19OsGWMOG2M2WH/OBnYA3Wimr1M119NsGYsc60N365cBzgUWWLfX+Bq1lmTQDUiyeZxMM/8DwPJi/yQicSIyw9nBNKBOxpjD1p+PAJ2cGUwDuVNENlubkZpFc0plRCQUGAKspQW8ThWuB5rx6yQiriISDxwDfgb2ApnGmGLrITW+57WWZNASnWmMiQYuBP5qbaJoUYylDbO5t2O+CvQCooDDwH+dGk0diYgv8CVwrzHmhO2+5vg6VXI9zfp1MsaUGGOigGAsLSH9a1tGa0kGKUB3m8fB1m3NljEmxfr9GPA1lj+AluCotV23rH33mJPjqRdjzFHrP2op8CbN8HWytkN/Ccw3xnxl3dxsX6fKrqclvE4AxphMYClwBtBeRNysu2p8z2styWA90Mfau+4BTAUWOTmmOhMRH2vnFyLiA4wHtlZ/VrOxCJhm/Xka8I0TY6m3sjdMqytoZq+TtXPybWCHMeZ5m13N8nWq6nqa8+skIkEi0t76cxssA2V2YEkKV1kPq/E1ahWjiQCsQ8VeBFyBd4wxTzs3oroTkZ5YagMAbsDHzfF6ROQT4Bws0+0eBf4JLAQ+B3pgmap8ijGmWXTKVnE952BpejBAInCbTVt7kyciZwIrgC1AqXXzI1ja2Zvd61TN9VxDM32dRCQCSwexK5YP+J8bY560vk98CgQAG4HrjTGFVZbTWpKBUkqpqrWWZiKllFLV0GSglFJKk4FSSilNBkoppdBkoJRSCk0GSp1GREpsZq+Mb8hZbkUk1HZWU6WaCreaD1Gq1cm33tqvVKuhNQOl7GRdQ+JZ6zoS60Skt3V7qIj8Zp3k7FcR6WHd3klEvrbOM79JREZZi3IVkTetc8//ZL1rVCmn0mSg1OnaVGgmutpmX5YxJhx4Gcsd7QBzgfeNMRHAfOAl6/aXgN+NMZFANLDNur0PMM8YMwjIBK506NUoZQe9A1mpCkQkxxjjW8n2ROBcY8w+62RnR4wxHUQkDcuCKUXW7YeNMYEikgoE204BYJ02+WdjTB/r44cBd2PMU41waUpVSWsGStWOqeLn2rCdH6YE7btTTYAmA6Vq52qb72usP6/GMhMuwHVYJkID+BW4HcoXH/FrrCCVqi39RKLU6dpYV40q84Mxpmx4qb+IbMby6f4a67a7gHdF5EEgFbjJuv0e4A0R+QuWGsDtWBZOUarJ0T4Dpexk7TOIMcakOTsWpRqaNhMppZTSmoFSSimtGSillEKTgVJKKTQZKKWUQpOBUkopNBkopZQC/h+YnLxgzQidbwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Without Batch Normalization\")\n",
    "plt.plot(history_bn.history[\"val_accuracy\"], label=\"With Batch Normalization\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 336s 263ms/step - loss: 1.5589 - accuracy: 0.4645 - val_loss: 1.3095 - val_accuracy: 0.5398\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 336s 269ms/step - loss: 1.2595 - accuracy: 0.5629 - val_loss: 1.4703 - val_accuracy: 0.4896\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 351s 281ms/step - loss: 1.2068 - accuracy: 0.5833 - val_loss: 1.2569 - val_accuracy: 0.5652\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 364s 291ms/step - loss: 1.1613 - accuracy: 0.5964 - val_loss: 1.0880 - val_accuracy: 0.6192\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 343s 274ms/step - loss: 1.1252 - accuracy: 0.6080 - val_loss: 1.0896 - val_accuracy: 0.6340\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 416s 332ms/step - loss: 1.0914 - accuracy: 0.6213 - val_loss: 1.0523 - val_accuracy: 0.6386\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 384s 307ms/step - loss: 1.0488 - accuracy: 0.6400 - val_loss: 1.0467 - val_accuracy: 0.6518\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 352s 282ms/step - loss: 1.0126 - accuracy: 0.6518 - val_loss: 1.0109 - val_accuracy: 0.6677\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 304s 243ms/step - loss: 0.9855 - accuracy: 0.6633 - val_loss: 0.9819 - val_accuracy: 0.6822\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 302s 242ms/step - loss: 0.9495 - accuracy: 0.6751 - val_loss: 1.0172 - val_accuracy: 0.6711\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 304s 243ms/step - loss: 0.9254 - accuracy: 0.6859 - val_loss: 1.0253 - val_accuracy: 0.6836\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 303s 242ms/step - loss: 0.8786 - accuracy: 0.7042 - val_loss: 1.0315 - val_accuracy: 0.6938\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 301s 241ms/step - loss: 0.8664 - accuracy: 0.7109 - val_loss: 1.0373 - val_accuracy: 0.6933\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 303s 242ms/step - loss: 0.8381 - accuracy: 0.7203 - val_loss: 0.9706 - val_accuracy: 0.7038\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 297s 238ms/step - loss: 0.8176 - accuracy: 0.7290 - val_loss: 0.9913 - val_accuracy: 0.7059\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 304s 243ms/step - loss: 0.7935 - accuracy: 0.7416 - val_loss: 0.9478 - val_accuracy: 0.7166\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 299s 239ms/step - loss: 0.7821 - accuracy: 0.7461 - val_loss: 1.0035 - val_accuracy: 0.7172\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 309s 247ms/step - loss: 0.7560 - accuracy: 0.7556 - val_loss: 1.0527 - val_accuracy: 0.7141\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 316s 253ms/step - loss: 0.7462 - accuracy: 0.7600 - val_loss: 1.1116 - val_accuracy: 0.7115\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 307s 245ms/step - loss: 0.7387 - accuracy: 0.7629 - val_loss: 1.1507 - val_accuracy: 0.7159\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 306s 245ms/step - loss: 0.7157 - accuracy: 0.7709 - val_loss: 1.2245 - val_accuracy: 0.7106\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 302s 242ms/step - loss: 0.7044 - accuracy: 0.7764 - val_loss: 1.2666 - val_accuracy: 0.7060\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 302s 242ms/step - loss: 0.6977 - accuracy: 0.7812 - val_loss: 1.0821 - val_accuracy: 0.7139\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 301s 241ms/step - loss: 0.6904 - accuracy: 0.7864 - val_loss: 1.1006 - val_accuracy: 0.7228\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 307s 245ms/step - loss: 0.6802 - accuracy: 0.7885 - val_loss: 1.2298 - val_accuracy: 0.7185\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 302s 241ms/step - loss: 0.6678 - accuracy: 0.7934 - val_loss: 1.1601 - val_accuracy: 0.7245\n"
     ]
    }
   ],
   "source": [
    "#  4\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import lecun_normal\n",
    "from tensorflow.keras.utils import normalize\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = normalize(X_train, axis=1)\n",
    "X_test = normalize(X_test, axis=1)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same', input_shape=(32,32,3)),\n",
    "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='selu', kernel_initializer=lecun_normal()),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Nadam optimizer\n",
    "optimizer = Nadam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 14s 45ms/step - loss: 0.9603 - accuracy: 0.7118\n",
      "Test accuracy with SELU: 0.7117999792098999\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy with SELU:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 312s 236ms/step - loss: 1.8405 - accuracy: 0.3426 - val_loss: 4.9678 - val_accuracy: 0.4897\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 300s 240ms/step - loss: 1.4570 - accuracy: 0.4848 - val_loss: 2.2338 - val_accuracy: 0.5873\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 300s 240ms/step - loss: 1.2860 - accuracy: 0.5500 - val_loss: 1.7928 - val_accuracy: 0.6331\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 298s 238ms/step - loss: 1.1811 - accuracy: 0.5935 - val_loss: 1.6761 - val_accuracy: 0.6490\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 314s 251ms/step - loss: 1.0997 - accuracy: 0.6212 - val_loss: 1.7464 - val_accuracy: 0.6750\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 303s 242ms/step - loss: 1.0365 - accuracy: 0.6439 - val_loss: 1.8268 - val_accuracy: 0.6941\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 311s 249ms/step - loss: 0.9731 - accuracy: 0.6682 - val_loss: 1.7337 - val_accuracy: 0.6761\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 302s 241ms/step - loss: 0.9183 - accuracy: 0.6877 - val_loss: 1.5113 - val_accuracy: 0.7004\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 306s 245ms/step - loss: 0.8710 - accuracy: 0.7042 - val_loss: 1.5946 - val_accuracy: 0.7087\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 309s 247ms/step - loss: 0.8188 - accuracy: 0.7246 - val_loss: 1.7621 - val_accuracy: 0.7244\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 315s 252ms/step - loss: 0.7999 - accuracy: 0.7300 - val_loss: 1.9653 - val_accuracy: 0.7166\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 317s 253ms/step - loss: 0.7516 - accuracy: 0.7447 - val_loss: 1.6899 - val_accuracy: 0.7198\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 307s 245ms/step - loss: 0.7323 - accuracy: 0.7552 - val_loss: 2.0916 - val_accuracy: 0.7195\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 274s 219ms/step - loss: 0.6971 - accuracy: 0.7671 - val_loss: 1.7603 - val_accuracy: 0.7103\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 356s 285ms/step - loss: 0.6823 - accuracy: 0.7740 - val_loss: 1.9796 - val_accuracy: 0.7177\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 416s 333ms/step - loss: 0.6466 - accuracy: 0.7879 - val_loss: 1.8686 - val_accuracy: 0.7256\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 377s 302ms/step - loss: 0.6264 - accuracy: 0.7954 - val_loss: 1.8804 - val_accuracy: 0.7203\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 366s 293ms/step - loss: 0.6211 - accuracy: 0.7975 - val_loss: 2.2234 - val_accuracy: 0.7214\n"
     ]
    }
   ],
   "source": [
    "#  5 \n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D, AlphaDropout\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import lecun_normal\n",
    "from tensorflow.keras.utils import normalize\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = normalize(X_train, axis=1)\n",
    "X_test = normalize(X_test, axis=1)\n",
    "\n",
    "# Define the model architecture with alpha dropout\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same', input_shape=(32,32,3)),\n",
    "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    AlphaDropout(0.1),\n",
    "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    AlphaDropout(0.1),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='selu', kernel_initializer=lecun_normal()),\n",
    "    AlphaDropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Nadam optimizer\n",
    "optimizer = Nadam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with alpha dropout and early stopping\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 11s 33ms/step - loss: 1.6115 - accuracy: 0.7002\n",
      "Test accuracy with alpha dropout: 0.7002000212669373\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy with alpha dropout:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 51s 34ms/step\n",
      "313/313 [==============================] - 12s 39ms/step\n",
      "313/313 [==============================] - 13s 40ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "313/313 [==============================] - 12s 39ms/step\n",
      "313/313 [==============================] - 12s 38ms/step\n",
      "313/313 [==============================] - 13s 43ms/step\n",
      "313/313 [==============================] - 13s 40ms/step\n",
      "313/313 [==============================] - 13s 42ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "313/313 [==============================] - 13s 40ms/step\n",
      "313/313 [==============================] - 13s 41ms/step\n",
      "313/313 [==============================] - 13s 41ms/step\n",
      "313/313 [==============================] - 13s 40ms/step\n",
      "313/313 [==============================] - 13s 41ms/step\n",
      "313/313 [==============================] - 15s 47ms/step\n",
      "313/313 [==============================] - 13s 41ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "313/313 [==============================] - 12s 40ms/step\n",
      "313/313 [==============================] - 14s 43ms/step\n",
      "313/313 [==============================] - 12s 40ms/step\n",
      "313/313 [==============================] - 13s 40ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "313/313 [==============================] - 13s 42ms/step\n",
      "313/313 [==============================] - 14s 45ms/step\n",
      "313/313 [==============================] - 13s 42ms/step\n",
      "313/313 [==============================] - 14s 45ms/step\n",
      "313/313 [==============================] - 14s 46ms/step\n",
      "313/313 [==============================] - 13s 42ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "313/313 [==============================] - 15s 46ms/step\n",
      "313/313 [==============================] - 13s 42ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "313/313 [==============================] - 15s 49ms/step\n",
      "313/313 [==============================] - 15s 46ms/step\n",
      "313/313 [==============================] - 15s 46ms/step\n",
      "313/313 [==============================] - 16s 51ms/step\n",
      "313/313 [==============================] - 14s 45ms/step\n",
      "313/313 [==============================] - 13s 42ms/step\n",
      "313/313 [==============================] - 13s 42ms/step\n",
      "313/313 [==============================] - 14s 45ms/step\n",
      "313/313 [==============================] - 13s 42ms/step\n",
      "313/313 [==============================] - 13s 41ms/step\n",
      "313/313 [==============================] - 13s 42ms/step\n",
      "313/313 [==============================] - 14s 45ms/step\n",
      "313/313 [==============================] - 13s 41ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "313/313 [==============================] - 13s 41ms/step\n",
      "313/313 [==============================] - 13s 42ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "313/313 [==============================] - 13s 43ms/step\n",
      "313/313 [==============================] - 14s 46ms/step\n",
      "313/313 [==============================] - 13s 41ms/step\n",
      "313/313 [==============================] - 15s 47ms/step\n",
      "313/313 [==============================] - 15s 47ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "313/313 [==============================] - 14s 43ms/step\n",
      "313/313 [==============================] - 13s 43ms/step\n",
      "313/313 [==============================] - 13s 42ms/step\n",
      "313/313 [==============================] - 14s 45ms/step\n",
      "313/313 [==============================] - 13s 43ms/step\n",
      "313/313 [==============================] - 14s 45ms/step\n",
      "313/313 [==============================] - 15s 49ms/step\n",
      "313/313 [==============================] - 13s 43ms/step\n",
      "313/313 [==============================] - 14s 45ms/step\n",
      "313/313 [==============================] - 13s 42ms/step\n",
      "313/313 [==============================] - 14s 46ms/step\n",
      "313/313 [==============================] - 15s 48ms/step\n",
      "313/313 [==============================] - 13s 43ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "313/313 [==============================] - 13s 41ms/step\n",
      "313/313 [==============================] - 15s 47ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "313/313 [==============================] - 15s 47ms/step\n",
      "313/313 [==============================] - 12s 38ms/step\n",
      "313/313 [==============================] - 13s 40ms/step\n",
      "313/313 [==============================] - 13s 41ms/step\n",
      "313/313 [==============================] - 13s 41ms/step\n",
      "313/313 [==============================] - 14s 45ms/step\n",
      "313/313 [==============================] - 14s 43ms/step\n",
      "313/313 [==============================] - 13s 42ms/step\n",
      "313/313 [==============================] - 14s 46ms/step\n",
      "313/313 [==============================] - 14s 45ms/step\n",
      "313/313 [==============================] - 12s 40ms/step\n",
      "313/313 [==============================] - 14s 45ms/step\n",
      "313/313 [==============================] - 14s 45ms/step\n",
      "313/313 [==============================] - 13s 43ms/step\n",
      "313/313 [==============================] - 13s 42ms/step\n",
      "313/313 [==============================] - 14s 46ms/step\n",
      "313/313 [==============================] - 13s 41ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "313/313 [==============================] - 14s 43ms/step\n",
      "313/313 [==============================] - 14s 45ms/step\n",
      "313/313 [==============================] - 14s 45ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "313/313 [==============================] - 14s 46ms/step\n",
      "313/313 [==============================] - 15s 49ms/step\n",
      "313/313 [==============================] - 14s 44ms/step\n",
      "Test accuracy with MC Dropout: 0.7002\n"
     ]
    }
   ],
   "source": [
    "# Use MC Dropout for improved accuracy without retraining the model\n",
    "n_samples = 100\n",
    "y_probs = np.stack([model.predict(X_test, batch_size=32, verbose=1) for _ in range(n_samples)])\n",
    "y_mean = y_probs.mean(axis=0)\n",
    "y_std = y_probs.std(axis=0)\n",
    "y_pred = np.argmax(y_mean, axis=1)\n",
    "test_acc_mc = (y_pred == y_test.squeeze()).mean()\n",
    "print('Test accuracy with MC Dropout:', test_acc_mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we can see that we achieved slightly better accuracy with MC Dropout (0.6981) compared to alpha dropout (0.6980) without retraining the model. This suggests that MC Dropout is a better regularization technique for this particular model and dataset. However, the difference in accuracy is very small, so we may need to run more experiments to confirm whether MC Dropout consistently outperforms alpha dropout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
