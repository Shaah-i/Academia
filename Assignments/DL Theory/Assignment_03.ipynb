{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment_03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
    "\n",
    "**Ans.** \n",
    "* No, all weights should be sampled independently; they should not all have the same initial value.\n",
    "* One important goal of sampling weights randomly is to break symmetry: if all the weights have the same initial value, even if that value is not zero, then symmetry is not broken  and backpropagation will be unable to break it.this means that all the neurons in any given layer will always have the same weights. It’s like having just one neuron per layer, and much slower.\n",
    "* It is virtually impossible for such a configuration to converge to a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. Is it OK to initialize the bias terms to 0?\n",
    "\n",
    "**Ans.** It is perfectly fine to initialize the bias terms to zero. Some people like to initialize them just like weights and that’s okay too it does not make much difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3. Name three advantages of the SELU activation function over ReLU.\n",
    "\n",
    "**Ans.** \n",
    "* It can take on negative values, so the average output of the neurons in any given layer is typically closer to zero than when using the ReLU activation function This helps alleviate the vanishing gradients problem.\n",
    "* It always has a nonzero derivative, which avoids the dying units issue that can affect ReLU units.\n",
    "* When the conditions are right then the SELU activation function ensures the model is self-normalized, which solves the exploding/vanishing gradients problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
    "\n",
    "**Ans.** \n",
    "* The SELU activation function is a good default.\n",
    "* If you need the neural network to be as fast as possible, you can use one of the leaky ReLU variants instead (e.g., a simple leaky ReLU using the default hyperparameter value).\n",
    "* Simplicity of the ReLU activation function makes it many people’s preferred option, despite the fact that it is generally outperformed by SELU and leaky ReLU. However, the ReLU activation function’s ability to output precisely zero can be useful in some cases  Moreover, it can sometimes benefit from optimized implementation as well as from hardware acceleration.\n",
    "* Hyperbolic tangent (tanh) can be useful in the output layer if you need to output a number between –1 and 1, but nowadays it is not used much in hidden layers (except in recurrent nets).\n",
    "* Logistic activation function is also useful in the output layer when you need to estimate a probability rare in hidden layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
    "\n",
    "**Ans.** If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed,moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again and so on. In this way many times before converging, so overall it will take much longer to converge than with a smaller momentum value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6. Name three ways you can produce a sparse model.\n",
    "\n",
    "**Ans.** \n",
    "* One way to produce a sparse model (i.e., with most weights equal to zero) is to train the model normally, then zero out tiny weights.\n",
    "* For more sparsity, you can apply ℓ1 regularization during training, which pushes the optimizer toward sparsity.\n",
    "* A third option is to use the TensorFlow Model Optimization Toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
    "\n",
    "**Ans.** One way to produce a sparse model (i.e., with most weights equal to zero) is to train the model normally, then zero out tiny weights.\n",
    "-For more sparsity, you can apply ℓ1 regularization during training, which pushes the optimizer toward sparsity.\n",
    "-A third option is to use the TensorFlow Model Optimization Toolkit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "\n",
    "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
    "\n",
    "b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "\n",
    "c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "\n",
    "d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "\n",
    "e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n",
    "\n",
    "**Ans.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1563/1563 [==============================] - 41s 16ms/step - loss: 1.9989 - accuracy: 0.2672 - val_loss: 1.8479 - val_accuracy: 0.3199\n",
      "Epoch 2/30\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.8202 - accuracy: 0.3367 - val_loss: 1.7906 - val_accuracy: 0.3450\n",
      "Epoch 3/30\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.7608 - accuracy: 0.3607 - val_loss: 1.7063 - val_accuracy: 0.3825\n",
      "Epoch 4/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.7166 - accuracy: 0.3793 - val_loss: 1.7183 - val_accuracy: 0.3938\n",
      "Epoch 5/30\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.6771 - accuracy: 0.4009 - val_loss: 1.6466 - val_accuracy: 0.4083\n",
      "Epoch 6/30\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 1.6468 - accuracy: 0.4105 - val_loss: 1.6846 - val_accuracy: 0.3944\n",
      "Epoch 7/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8502 - accuracy: 0.3482 - val_loss: 2.0825 - val_accuracy: 0.1804\n",
      "Epoch 8/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.0008 - accuracy: 0.2275 - val_loss: 1.9622 - val_accuracy: 0.2692\n",
      "Epoch 9/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8686 - accuracy: 0.2938 - val_loss: 1.8534 - val_accuracy: 0.3041\n",
      "Epoch 10/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7799 - accuracy: 0.3333 - val_loss: 1.7591 - val_accuracy: 0.3576\n",
      "Epoch 11/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7227 - accuracy: 0.3665 - val_loss: 1.7095 - val_accuracy: 0.3721\n",
      "Epoch 12/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6701 - accuracy: 0.3918 - val_loss: 1.6787 - val_accuracy: 0.3868\n",
      "Epoch 13/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6889 - accuracy: 0.3852 - val_loss: 1.7149 - val_accuracy: 0.3693\n",
      "Epoch 14/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7845 - accuracy: 0.3319 - val_loss: 1.8399 - val_accuracy: 0.3005\n",
      "Epoch 15/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7766 - accuracy: 0.3286 - val_loss: 1.6915 - val_accuracy: 0.3727\n",
      "Epoch 16/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6740 - accuracy: 0.3826 - val_loss: 1.6374 - val_accuracy: 0.3994\n",
      "Epoch 17/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6206 - accuracy: 0.4045 - val_loss: 1.5949 - val_accuracy: 0.4221\n",
      "Epoch 18/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.5836 - accuracy: 0.4263 - val_loss: 1.6405 - val_accuracy: 0.4029\n",
      "Epoch 19/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.2199 - accuracy: 0.3743 - val_loss: 1.7060 - val_accuracy: 0.3792\n",
      "Epoch 20/30\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6342 - accuracy: 0.4026 - val_loss: 1.6772 - val_accuracy: 0.3920\n",
      "Epoch 21/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.5935 - accuracy: 0.4165 - val_loss: 1.6156 - val_accuracy: 0.4076\n",
      "Epoch 22/30\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.5724 - accuracy: 0.4269 - val_loss: 1.6134 - val_accuracy: 0.4351\n",
      "Epoch 23/30\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 1.5565 - accuracy: 0.4343 - val_loss: 1.6334 - val_accuracy: 0.4156\n",
      "Epoch 24/30\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 1.7067 - accuracy: 0.3694 - val_loss: 1.7403 - val_accuracy: 0.3441\n",
      "Epoch 25/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.6615 - accuracy: 0.3887 - val_loss: 1.6576 - val_accuracy: 0.3961\n",
      "Epoch 26/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.5906 - accuracy: 0.4175 - val_loss: 1.6033 - val_accuracy: 0.4048\n",
      "Epoch 27/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.5594 - accuracy: 0.4341 - val_loss: 1.5728 - val_accuracy: 0.4309\n",
      "Epoch 28/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.5349 - accuracy: 0.4440 - val_loss: 1.5928 - val_accuracy: 0.4237\n",
      "Epoch 29/30\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.5118 - accuracy: 0.4534 - val_loss: 1.5339 - val_accuracy: 0.4545\n",
      "Epoch 30/30\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.4960 - accuracy: 0.4615 - val_loss: 1.5531 - val_accuracy: 0.4387\n"
     ]
    }
   ],
   "source": [
    "#  1\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Build the model\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='he_normal', activation='elu'))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=30, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Convert pixel values to float and normalize\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.10029999911785126\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1407/1407 [==============================] - 73s 45ms/step - loss: 1.2899 - accuracy: 0.5455 - val_loss: 1.3266 - val_accuracy: 0.5366\n",
      "Epoch 2/50\n",
      "1407/1407 [==============================] - 62s 44ms/step - loss: 0.9646 - accuracy: 0.6614 - val_loss: 1.8943 - val_accuracy: 0.4526\n",
      "Epoch 3/50\n",
      "1407/1407 [==============================] - 65s 46ms/step - loss: 0.8303 - accuracy: 0.7112 - val_loss: 1.0210 - val_accuracy: 0.6524\n",
      "Epoch 4/50\n",
      "1407/1407 [==============================] - 74s 53ms/step - loss: 0.7341 - accuracy: 0.7457 - val_loss: 1.0368 - val_accuracy: 0.6496\n",
      "Epoch 5/50\n",
      "1407/1407 [==============================] - 62s 44ms/step - loss: 0.6501 - accuracy: 0.7719 - val_loss: 1.1812 - val_accuracy: 0.6290\n",
      "Epoch 6/50\n",
      "1407/1407 [==============================] - 75s 53ms/step - loss: 0.5779 - accuracy: 0.7989 - val_loss: 1.0426 - val_accuracy: 0.6592\n"
     ]
    }
   ],
   "source": [
    "#  3\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Convert pixel values to float and normalize\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Define the neural network architecture with Batch Normalization\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Set up early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history_bn = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with Batch Normalization: 0.6371999979019165\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss_bn, test_acc_bn = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test accuracy with Batch Normalization:\", test_acc_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Without Batch Normalization\")\n",
    "plt.plot(history_bn.history[\"val_accuracy\"], label=\"With Batch Normalization\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 304s 222ms/step - loss: 1.5228 - accuracy: 0.4763 - val_loss: 1.3080 - val_accuracy: 0.5482\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 282s 225ms/step - loss: 1.2621 - accuracy: 0.5637 - val_loss: 1.1522 - val_accuracy: 0.6023\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 481s 385ms/step - loss: 1.2050 - accuracy: 0.5837 - val_loss: 1.1951 - val_accuracy: 0.5848\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 318s 255ms/step - loss: 1.1657 - accuracy: 0.5985 - val_loss: 1.1069 - val_accuracy: 0.6130\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 296s 237ms/step - loss: 1.1276 - accuracy: 0.6094 - val_loss: 1.1702 - val_accuracy: 0.6052\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 142310s 114s/step - loss: 1.0871 - accuracy: 0.6231 - val_loss: 1.0503 - val_accuracy: 0.6383\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 318s 255ms/step - loss: 1.0541 - accuracy: 0.6378 - val_loss: 1.0994 - val_accuracy: 0.6421\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 333s 266ms/step - loss: 1.0234 - accuracy: 0.6495 - val_loss: 1.0345 - val_accuracy: 0.6540\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 315s 252ms/step - loss: 0.9770 - accuracy: 0.6660 - val_loss: 0.9793 - val_accuracy: 0.6708\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 299s 240ms/step - loss: 0.9540 - accuracy: 0.6740 - val_loss: 0.9676 - val_accuracy: 0.6861\n",
      "Epoch 11/100\n",
      " 763/1250 [=================>............] - ETA: 1:36 - loss: 0.9049 - accuracy: 0.6925"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function UniquePtr.__del__ at 0x000001F2B22C9360>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\HappySoul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\c_api_util.py\", line 74, in __del__\n",
      "    self.deleter(obj)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 42>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Train the model with early stopping\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1656\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1654\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[0;32m   1655\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1656\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1658\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:476\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \n\u001b[0;32m    471\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 476\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:323\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 323\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    328\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:346\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    343\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    349\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:394\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    393\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 394\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:1094\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1094\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:1171\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1169\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m     logs \u001b[38;5;241m=\u001b[39m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m-> 1171\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\generic_utils.py:297\u001b[0m, in \u001b[0;36mProgbar.update\u001b[1;34m(self, current, values, finalize)\u001b[0m\n\u001b[0;32m    294\u001b[0m         info \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    296\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m info\n\u001b[1;32m--> 297\u001b[0m     \u001b[43mio_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\io_utils.py:80\u001b[0m, in \u001b[0;36mprint_msg\u001b[1;34m(message, line_break)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(message)\n\u001b[1;32m---> 80\u001b[0m     \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(message)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\iostream.py:487\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    485\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[0;32m    489\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:600\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    598\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 600\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#  4\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import lecun_normal\n",
    "from tensorflow.keras.utils import normalize\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = normalize(X_train, axis=1)\n",
    "X_test = normalize(X_test, axis=1)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same', input_shape=(32,32,3)),\n",
    "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='selu', kernel_initializer=lecun_normal()),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Nadam optimizer\n",
    "optimizer = Nadam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy with SELU:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  5 \n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D, AlphaDropout\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import lecun_normal\n",
    "from tensorflow.keras.utils import normalize\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load the CIFAR10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = normalize(X_train, axis=1)\n",
    "X_test = normalize(X_test, axis=1)\n",
    "\n",
    "# Define the model architecture with alpha dropout\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same', input_shape=(32,32,3)),\n",
    "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    AlphaDropout(0.1),\n",
    "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    AlphaDropout(0.1),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='selu', kernel_initializer=lecun_normal()),\n",
    "    AlphaDropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with Nadam optimizer\n",
    "optimizer = Nadam(lr=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model with alpha dropout and early stopping\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy with alpha dropout:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MC Dropout for improved accuracy without retraining the model\n",
    "n_samples = 100\n",
    "y_probs = np.stack([model.predict(X_test, batch_size=32, verbose=1) for _ in range(n_samples)])\n",
    "y_mean = y_probs.mean(axis=0)\n",
    "y_std = y_probs.std(axis=0)\n",
    "y_pred = np.argmax(y_mean, axis=1)\n",
    "test_acc_mc = (y_pred == y_test.squeeze()).mean()\n",
    "print('Test accuracy with MC Dropout:', test_acc_mc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we can see that we achieved slightly better accuracy with MC Dropout (0.6981) compared to alpha dropout (0.6980) without retraining the model. This suggests that MC Dropout is a better regularization technique for this particular model and dataset. However, the difference in accuracy is very small, so we may need to run more experiments to confirm whether MC Dropout consistently outperforms alpha dropout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
