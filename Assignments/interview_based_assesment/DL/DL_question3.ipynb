{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vzq7_BtIEtut"
      },
      "source": [
        "## Pytorch"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MwiqjHtm_lMC"
      },
      "source": [
        "Load libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uQgdtRbK-wG1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "J79kSL8E_pvf"
      },
      "source": [
        "CNN architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "Ww94X4dFVs9N"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(1, 64, kernel_size=3),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(64, 128, kernel_size=3),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2),\n",
        "    nn.Dropout2d(0.25),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(128 * 12 * 12, 512),  # Increase the size of the fully connected layer\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, 10)\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy_yEdTq_uEy"
      },
      "source": [
        "Dataset and data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "axiUMDl1-_BB"
      },
      "outputs": [],
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load MNIST dataset and apply transforms\n",
        "train_dataset = MNIST(root='.', train=True, download=True, transform=ToTensor())\n",
        "test_dataset = MNIST(root='.', train=False, download=True, transform=ToTensor())\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 128\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MZYYj5JC_4CL"
      },
      "source": [
        "Model, lossfunction and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "ArZlizNoAAMu"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the CNN model\n",
        "# model = CNN()\n",
        "\n",
        "# Define loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aBbVeGWnAJVQ"
      },
      "source": [
        "Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91klaq-fAGGx",
        "outputId": "9f52aa76-bcb9-4bd6-b1be-affb8a8807ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/15], Train Loss: 0.1510, Val Accuracy: 98.56%\n",
            "Epoch [2/15], Train Loss: 0.0521, Val Accuracy: 98.85%\n",
            "Epoch [3/15], Train Loss: 0.0368, Val Accuracy: 99.04%\n",
            "Epoch [4/15], Train Loss: 0.0278, Val Accuracy: 98.77%\n",
            "Epoch [5/15], Train Loss: 0.0210, Val Accuracy: 99.02%\n",
            "Epoch [6/15], Train Loss: 0.0183, Val Accuracy: 99.03%\n",
            "Epoch [7/15], Train Loss: 0.0161, Val Accuracy: 99.26%\n",
            "Epoch [8/15], Train Loss: 0.0131, Val Accuracy: 99.22%\n",
            "Epoch [9/15], Train Loss: 0.0119, Val Accuracy: 99.27%\n",
            "Epoch [10/15], Train Loss: 0.0112, Val Accuracy: 99.26%\n",
            "Epoch [11/15], Train Loss: 0.0101, Val Accuracy: 99.34%\n",
            "Epoch [12/15], Train Loss: 0.0095, Val Accuracy: 99.21%\n",
            "Epoch [13/15], Train Loss: 0.0090, Val Accuracy: 99.15%\n",
            "Epoch [14/15], Train Loss: 0.0063, Val Accuracy: 99.18%\n",
            "Epoch [15/15], Train Loss: 0.0078, Val Accuracy: 99.15%\n"
          ]
        }
      ],
      "source": [
        "# Set the number of epochs\n",
        "num_epochs = 15\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    \n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item() * images.size(0)\n",
        "    \n",
        "    # Calculate average training loss\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    \n",
        "    # Evaluate the model on the test set\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    # Calculate validation accuracy\n",
        "    val_accuracy = 100 * correct / total\n",
        "    \n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "    \n",
        "    # Break the training loop if desired accuracy is reached\n",
        "    if val_accuracy >= 99.40:\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PZwFkwBGE2IA"
      },
      "source": [
        "## Tensorflow\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "3_i_lBSLE90z"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sOoiHKHRJm-c"
      },
      "source": [
        "Load and preprocess the MNIST dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "2eib5SulJscr"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FiafB7sgKFfq"
      },
      "source": [
        "Define the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "RjXs5RV4KGs-"
      },
      "outputs": [],
      "source": [
        "# model = Sequential([\n",
        "#     Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "#     MaxPooling2D((2, 2)),\n",
        "#     Conv2D(32, (3, 3), activation='relu'),\n",
        "#     MaxPooling2D((2, 2)),\n",
        "#     Flatten(),\n",
        "#     Dense(128, activation='relu'),\n",
        "#     Dense(10, activation='softmax')\n",
        "# ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "nh_Ad3InLENE"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),  # Adding dropout regularization\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7OcfCnGJKJm3"
      },
      "source": [
        " Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "khPiafbKKKeU"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.00001\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JM7vqPR-KOge"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4jSMFi2KPgi",
        "outputId": "42f151f5-a85e-4a41-e847-d50a1dabbe65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "422/422 [==============================] - 4s 5ms/step - loss: 0.0087 - accuracy: 0.9970 - val_loss: 0.0346 - val_accuracy: 0.9925\n",
            "Epoch 2/20\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.0073 - accuracy: 0.9974 - val_loss: 0.0355 - val_accuracy: 0.9927\n",
            "Epoch 3/20\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 0.0361 - val_accuracy: 0.9923\n",
            "Epoch 4/20\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.0366 - val_accuracy: 0.9922\n",
            "Epoch 5/20\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.0372 - val_accuracy: 0.9920\n",
            "Epoch 6/20\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.0373 - val_accuracy: 0.9925\n",
            "Epoch 7/20\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0378 - val_accuracy: 0.9925\n",
            "Epoch 8/20\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.0380 - val_accuracy: 0.9923\n",
            "Epoch 9/20\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.0383 - val_accuracy: 0.9925\n",
            "Epoch 10/20\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.0388 - val_accuracy: 0.9925\n",
            "Epoch 11/20\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0389 - val_accuracy: 0.9923\n",
            "Epoch 12/20\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.0390 - val_accuracy: 0.9927\n",
            "Epoch 13/20\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0395 - val_accuracy: 0.9925\n",
            "Epoch 14/20\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.0048 - accuracy: 0.9983 - val_loss: 0.0394 - val_accuracy: 0.9922\n",
            "Epoch 15/20\n",
            "422/422 [==============================] - 2s 6ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.0399 - val_accuracy: 0.9925\n",
            "Epoch 16/20\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.0401 - val_accuracy: 0.9922\n",
            "Epoch 17/20\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0401 - val_accuracy: 0.9920\n",
            "Epoch 18/20\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0402 - val_accuracy: 0.9923\n",
            "Epoch 19/20\n",
            "422/422 [==============================] - 2s 4ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0403 - val_accuracy: 0.9927\n",
            "Epoch 20/20\n",
            "422/422 [==============================] - 2s 5ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0406 - val_accuracy: 0.9928\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x_train, y_train, validation_split=0.1, epochs=20, batch_size=128)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sdth2e9qKTBB"
      },
      "source": [
        "Evaluate the model on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhDzdM4BKUCp",
        "outputId": "5235c876-c7f9-43a2-a1bb-2e6106aa98bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0267 - accuracy: 0.9945\n",
            "Test Loss: 0.0267\n",
            "Test Accuracy: 0.9945\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
